{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-step prediction of river flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "# from tensorflow.nn import dynamic_rnn\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp # import the inspect_checkpoint library\n",
    "# import tensorboard as tb\n",
    "import numpy as np\n",
    "import scipy.io as sio # for working with .mat files\n",
    "# import pandas as pd # for working with .xlsx files\n",
    "from openpyxl import load_workbook # for working with .xlsx files\n",
    "import matplotlib.pyplot as plt # for plotting the data\n",
    "from datetime import datetime # for keeping separate TB logs for each run\n",
    "import os, sys\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180509-012821\n"
     ]
    }
   ],
   "source": [
    "# define unique name for log directory\n",
    "now = datetime.now()\n",
    "logdir = \"./logs/multi-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "print(now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "# define net parameters\n",
    "class params:\n",
    "    # initialization of instance variables\n",
    "    def __init__(self,n_lstm_layers=2,hidden_size=10,delay=10,pred_step=1,batch_size=5,n_epochs=100,stop_epochs=20,init_lr=0.001):\n",
    "        self.input_size = 1 # number of input features (we only follow one variable == flow)\n",
    "        self.num_classes = 1 # number of output classes (we wan't specific value, not classes, so this is 1)\n",
    "        self.target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "        self.n_lstm_layers = n_lstm_layers # number of vertically stacked LSTM layers\n",
    "        self.hidden_size = hidden_size # hidden state vector size in LSTM cell\n",
    "        self.delay = delay # the number of time-steps from which we are going to predict the next step\n",
    "        self.pred_step = pred_step # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "        self.batch_size = batch_size # number of inputs in one batch\n",
    "        self.n_epochs = n_epochs # number of epochs\n",
    "        self.stop_epochs = stop_epochs # if the loss value doesn't improve over the last stop_epochs, the training process will stop\n",
    "        self.init_lr = init_lr # initial learning rate for Adam optimizer (training algorithm)\n",
    "        self.net_unroll_size = delay + pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "        \n",
    "    # how will the class object be represented in string form (eg. when called with print())\n",
    "    def __str__(self):\n",
    "        answer = '''\n",
    "Input size ...................... {:4d}\n",
    "Number of classes ............... {:4d}\n",
    "Target shift .................... {:4d}\n",
    "Number of stacked LSTM layers ... {:4d}\n",
    "Hidden state size ............... {:4d}\n",
    "Delay ........................... {:4d}\n",
    "Number of prediciton steps....... {:4d}\n",
    "Batch size ...................... {:4d}\n",
    "Maximum number of epochs ........ {:4d}\n",
    "Early stopping epochs ........... {:4d}\n",
    "Initial learning rate ........... {:9.4f}'''.format(self.input_size\n",
    "                                              ,self.num_classes\n",
    "                                              ,self.target_shift\n",
    "                                              ,self.n_lstm_layers\n",
    "                                              ,self.hidden_size\n",
    "                                              ,self.delay\n",
    "                                              ,self.pred_step\n",
    "                                              ,self.batch_size\n",
    "                                              ,self.n_epochs\n",
    "                                              ,self.stop_epochs\n",
    "                                              ,self.init_lr)\n",
    "        return answer\n",
    "\n",
    "# net and training parameter specification\n",
    "par = params(n_lstm_layers=2\n",
    "            ,hidden_size=64\n",
    "            ,delay=256\n",
    "            ,pred_step=4\n",
    "            ,batch_size=4 # LOWER batch_size is better (https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/)\n",
    "            ,n_epochs=1000\n",
    "            ,stop_epochs=20\n",
    "            ,init_lr=0.001)\n",
    "    \n",
    "# FIGHTING OVERFITTING:\n",
    "# enable/disable input and output dropout in LSTM layer\n",
    "dropout = False\n",
    "# enable/disable L2 regularization\n",
    "regularization = False # NOT IMPLEMENTED YET (go to: https://stackoverflow.com/questions/37869744/tensorflow-lstm-regularization)\n",
    "\n",
    "# continue training the model with saved variables from previeous training\n",
    "continueLearning = False\n",
    "\n",
    "# decaying learning rate constants (for exponential decay)\n",
    "# Adam already has adaptive learning rate for individual weights \n",
    "# but it can be combined with decaying learning rate for faster convergence\n",
    "decay_steps = par.n_epochs//10 # every \"n_epochs//10\" epochs the learning rate is reduced\n",
    "decay_rate = 1 # the base of the exponential (rate of the decay ... 1 == no decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) DATA from prof. A. Procházka:\n",
    "* **url:**: http://uprt.vscht.cz/prochazka/pedag/Data/dataNN.zip\n",
    "* **name**: Weekly Labe river flow\n",
    "* **Provider source:** Prof. Ing. Aleš Procházka, CSc\n",
    "* **Span:** 313 weeks ~ 6 years of data\n",
    "* **Data size:** 313 values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# load data from Q.mat\n",
    "filename = './datasets/Q.MAT'\n",
    "data = sio.loadmat(filename) # samples were gathered with period of one week\n",
    "\n",
    "# convert to np array\n",
    "data = np.array(data['Q'],dtype=np.float32)\n",
    "\n",
    "print(np.shape(data))\n",
    "\n",
    "# normalize the data to interval (0,1)\n",
    "min_data = np.min(data)\n",
    "max_data = np.max(data)\n",
    "# data = np.divide(np.subtract(data,min_data),np.subtract(max_data,min_data)).flatten()\n",
    "# normalize the data to interval (-1,1) (cca 0 mean and 1 variance)\n",
    "mean_data = np.mean(data) # mean\n",
    "std_data = np.std(data) # standard deviation\n",
    "data = np.divide(np.subtract(data,mean_data),std_data).flatten()\n",
    "\n",
    "# divide the data into training, testing and validation part\n",
    "weeks_in_year = 52.1775\n",
    "years_in_data = 313/weeks_in_year\n",
    "\n",
    "years_in_train = int(years_in_data*0.7) # 70% of data rounded to the number of years\n",
    "years_in_test = int(np.ceil(years_in_data*0.15)) # 15% of data rounded to the number of years\n",
    "\n",
    "weeks_train = int(years_in_train*weeks_in_year) # number of weeks in training data\n",
    "weeks_test = int(years_in_test*weeks_in_year) # number of weeks in testing data\n",
    "\n",
    "end_of_train = weeks_train\n",
    "end_of_test = weeks_train + weeks_test\n",
    "\n",
    "x_train = data[:end_of_train]\n",
    "x_test = data[end_of_train:end_of_test]\n",
    "x_validation = data[end_of_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) DATA from Time Series Data Library:\n",
    "* **url:** https://datamarket.com/data/set/235a/mean-daily-saugeen-river-flows-jan-01-1915-to-dec-31-1979#!ds=235a&display=line\n",
    "* **name:** Mean daily Saugeen River (Canada) flows\n",
    "* **Provider source:** Hipel and McLeod (1994)\n",
    "* **Span:** Jan 01, 1915 to Dec 31, 1979\n",
    "* **Data size:** 23741 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mean daily saugeen River flows,']\n",
      "(1, 23741)\n",
      "(3652,)\n"
     ]
    }
   ],
   "source": [
    "# load excel spreadsheet with openpyxl:\n",
    "filename = './datasets/sugeen-river-flows.xlsx'\n",
    "xl = load_workbook(filename)\n",
    "\n",
    "# print sheet names:\n",
    "print(xl.get_sheet_names())\n",
    "\n",
    "# get sheet:\n",
    "sheet = xl.get_sheet_by_name('Mean daily saugeen River flows,')\n",
    "\n",
    "data = []\n",
    "\n",
    "# fill a list with values from cells:\n",
    "for cell in sheet['B16:B23756']:\n",
    "    data.append(cell[0].value)\n",
    "    \n",
    "# convert list to numpy array and reshape to a column vector\n",
    "data = np.array(data)\n",
    "data = np.reshape(data,(1,-1))\n",
    "\n",
    "print(np.shape(data))\n",
    "\n",
    "# normalize the data to interval (0,1) <- DONT!\n",
    "min_data = np.min(data)\n",
    "max_data = np.max(data)\n",
    "# data = np.divide(np.subtract(data,min_data),np.subtract(max_data,min_data)).flatten()\n",
    "# !!! CENTERING data:\n",
    "# normalize the data to interval (-1,1) (cca 0 mean and 1 variance)\n",
    "# data = data[0,:120]\n",
    "mean_data = np.mean(data) # mean\n",
    "std_data = np.std(data) # standard deviation\n",
    "data = np.divide(np.subtract(data,mean_data),std_data).flatten()\n",
    "\n",
    "# divide the data into training, testing and validation part\n",
    "days_in_data = np.shape(data)[0]\n",
    "days_in_year = 365.25\n",
    "years_in_data = days_in_data/days_in_year\n",
    "\n",
    "years_in_train = int(years_in_data*0.7) # 70% of data rounded to the number of years\n",
    "years_in_test = int(np.ceil(years_in_data*0.15)) # 15% of data rounded to the number of years\n",
    "\n",
    "days_train = int(years_in_train*days_in_year) # number of days in training data\n",
    "days_test = int(years_in_test*days_in_year) # number of days in testing data\n",
    "\n",
    "end_of_train = days_train\n",
    "end_of_test = days_train + days_test\n",
    "\n",
    "x_train = data[:end_of_train]\n",
    "x_test = data[end_of_train:end_of_test]\n",
    "x_validation = data[end_of_test:]\n",
    "\n",
    "print(np.shape(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the shifted time-series (targets)\n",
    "y_train = np.roll(x_train, par.target_shift)\n",
    "y_test = np.roll(x_test, par.target_shift)\n",
    "y_validation = np.roll(x_validation, par.target_shift)\n",
    "\n",
    "# delete the first elements of the time series that were reintroduced from the end of the timeseries\n",
    "y_train[:par.target_shift] = 0\n",
    "y_test[:par.target_shift] = 0\n",
    "y_validation[:par.target_shift] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset TensorFlow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# define tensorflow constants\n",
    "min_of_data = tf.constant(min_data, dtype=tf.float32, name='min_of_data')\n",
    "max_of_data = tf.constant(max_data, dtype=tf.float32, name='max_of_data')\n",
    "mean_of_data = tf.constant(mean_data, dtype=tf.float32, name='mean_of_data')\n",
    "std_of_data = tf.constant(std_data, dtype=tf.float32, name='std_of_data')\n",
    "\n",
    "# define output weights and biases\n",
    "with tf.name_scope(\"output_layer\"):\n",
    "    weights_out = tf.Variable(tf.random_normal([par.hidden_size,par.num_classes]),name='weights_out')\n",
    "    bias_out = tf.Variable(tf.random_normal([par.num_classes]),name='biases_out')\n",
    "\n",
    "# define placeholders for the batches of time-series\n",
    "x = tf.placeholder(tf.float32,[None, par.net_unroll_size, par.input_size],name='x') # batch of inputs\n",
    "y = tf.placeholder(tf.float32,[None, par.num_classes, par.pred_step],name='y') # batch of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# processing the input tensor from [par.batch_size,par.delay,par.input_size] to \"par.delay\" number of [par.batch_size,par.input_size] tensors\n",
    "input=tf.unstack(x, par.net_unroll_size, 1, name='LSTM_input_list') # create list of values by unstacking one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an LSTM cell:\n",
    "def make_cell(hidden_size):\n",
    "    return rnn.LSTMCell(hidden_size, state_is_tuple=True, activation=tf.tanh)\n",
    "\n",
    "# define an LSTM network with 'par.n_lstm_layers' layers\n",
    "with tf.name_scope(\"LSTM_layer\"):\n",
    "    lstm_cells = rnn.MultiRNNCell([make_cell(par.hidden_size) for _ in range(par.n_lstm_layers)], state_is_tuple=True)\n",
    "    \n",
    "    # add dropout to the inputs and outputs of the LSTM cell (reduces overfitting)\n",
    "    if dropout == True:\n",
    "        lstm_cells = rnn.DropoutWrapper(lstm_cells, input_keep_prob=0.5, output_keep_prob=0.5, state_keep_prob=1.0)\n",
    "    \n",
    "    # create static RNN from lstm_cell\n",
    "    outputs,_ = rnn.static_rnn(lstm_cells, input, dtype=tf.float32)\n",
    "#    outputs,_ = tf.nn.dynamic_rnn(lstm_cells, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of predictions based on the last \"par.pred_step\" time-step outputs (multi-step prediction)\n",
    "prediction = [tf.matmul(outputs[-i-1],weights_out) + bias_out for i in (range(par.pred_step))] # newest prediction first\n",
    "prediction = prediction[::-1] #reverse the list (oldest prediction first)\n",
    "\n",
    "prediction = tf.reshape(prediction,[par.batch_size,par.num_classes,par.pred_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define loss function with regularization\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "with tf.name_scope(\"loss\"):\n",
    "#    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "    loss = tf.sqrt(tf.losses.mean_squared_error(predictions=prediction,labels=y)) # RMSE (root mean squared error)\n",
    "\n",
    "# exponential decay of learning rate with epochs\n",
    "global_step = tf.Variable(1, trainable=False, name='global_step') # variable that keeps track of the step at which we are in the training\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1,name='increment_global_step') # operation that increments global step by one\n",
    "# decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "learning_rate = tf.train.exponential_decay(par.init_lr, global_step,\n",
    "                                           decay_steps, decay_rate, staircase=True) # decay at discrete intervals\n",
    "\n",
    "# define Adam optimizer for training of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# denormalize data (from (0,1)):\n",
    "# denormalized_prediction = min_of_data + tf.multiply(prediction, tf.subtract(max_of_data,min_of_data))\n",
    "# denormalized_y = min_of_data + tf.multiply(y, tf.subtract(max_of_data,min_of_data))\n",
    "\n",
    "# denormalize data (from (-1,1)): <- better\n",
    "denormalized_prediction = mean_of_data + tf.multiply(prediction, std_of_data)\n",
    "denormalized_y = mean_of_data + tf.multiply(y, std_of_data)\n",
    "\n",
    "# calculate relative error of denormalized data:\n",
    "with tf.name_scope(\"relative_error\"):\n",
    "    relative_error = tf.reduce_mean(tf.divide(tf.abs(tf.subtract(denormalized_prediction,denormalized_y)),denormalized_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorBoard summaries (visualization logs)\n",
    "# histogram summary for output weights\n",
    "w_out_summ = tf.summary.histogram(\"w_out_summary\", weights_out)\n",
    "# scalar summary for loss function\n",
    "# training_loss_summ = tf.summary.scalar(\"training_loss\",loss)\n",
    "# testing_loss_summ = tf.summary.scalar(\"testing_loss\",loss)\n",
    "# scalar summary for relative error\n",
    "# training_error_summ = tf.summary.scalar(\"relative_training_error\",relative_error)\n",
    "# testing_error_summ = tf.summary.scalar(\"relative_testing_error\",relative_error)\n",
    "\n",
    "# NOT USEFUL HERE\n",
    "# merge the summaries of all tf.summary calls (for TensorBoard visualization)\n",
    "# merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for generating the time-series batches\n",
    "def create_batch(x_data,y_data,batch_size,index):\n",
    "    \n",
    "    x_batch = np.zeros([batch_size,par.delay,par.input_size])\n",
    "    x_pad = np.zeros([batch_size,par.pred_step-1,par.input_size])\n",
    "    y_batch = np.zeros([batch_size,par.num_classes,par.pred_step])\n",
    "    \n",
    "    step = index*(batch_size*par.pred_step)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_batch[i,:,:] = np.reshape(x_data[step+i*par.pred_step:step+i*par.pred_step+par.delay],(par.delay,par.num_classes))\n",
    "        y_batch[i,:] = np.reshape(y_data[step+par.delay+i*par.pred_step+1:step+par.delay+i*par.pred_step+1+par.pred_step],(1,par.num_classes,-1))\n",
    "\n",
    "    # the last \"par.pred_step - 1\" columns in x_batch are padded with 0\n",
    "    # because there are no inputs into the net at these time steps\n",
    "    x_batch = np.hstack((x_batch, x_pad))\n",
    "    \n",
    "#    print(x_batch)\n",
    "#    print('________________')\n",
    "#    print(y_batch)\n",
    "#    print('================')\n",
    "    \n",
    "    return x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_model(inputs,labels,n_iter,save,train=False):\n",
    "    # Here are the important things that need to happen while running the model\n",
    "    \n",
    "    prediction_list = [] # list for prediction results\n",
    "    loss_val_sum = 0 # sum of the loss function throughout the whole data\n",
    "    error_val_mean = 0\n",
    "    prefix = \"\"\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # training batch\n",
    "        x_batch, y_batch = create_batch(inputs,labels,par.batch_size,i)\n",
    "\n",
    "        feed_dict = {x: x_batch, y: y_batch}\n",
    "        \n",
    "        # train the net on the data\n",
    "        if train:\n",
    "            session.run(optimizer,feed_dict=feed_dict) # run the optimization on the current batch\n",
    "            prefix = \"Training_\" # for summary writer\n",
    "        else:\n",
    "            prefix = \"Testing_\" # for summary writer\n",
    "\n",
    "        loss_val, prediction_val, error_val = session.run((loss, denormalized_prediction, relative_error), feed_dict=feed_dict)\n",
    "        \n",
    "        # prediction_val is a list of length \"par.pred_step\" with arrays of \"par.batch_size\" output values\n",
    "        # convert to numpy array of shape (par.batch_size, par.pred_step)\n",
    "        prediction_val = np.array(prediction_val)\n",
    "        # reshape the array to a vector of shape (1, par.pred_step*par.batch_size)\n",
    "        prediction_val = np.reshape(prediction_val, (1, par.pred_step*par.batch_size))\n",
    "        \n",
    "        loss_val_sum += loss_val # sum the losses across the batches\n",
    "        \n",
    "        # mean of prediction error values:\n",
    "        if i == 0:\n",
    "            error_val_mean = error_val\n",
    "        else:\n",
    "            error_val_mean = (error_val_mean + error_val)/2\n",
    "            \n",
    "        # save the results\n",
    "        if save:\n",
    "            # save the batch predictions to a list\n",
    "            prediction_list.extend(prediction_val[0,:])\n",
    "            \n",
    "    # the mean value of loss (throughout all the batches) in current epoch \n",
    "    loss_val_mean = loss_val_sum/n_iter\n",
    "        \n",
    "    # Create a new Summary object for sum of losses and mean of errors\n",
    "    loss_summary = tf.Summary()\n",
    "    error_summary = tf.Summary()\n",
    "    loss_summary.value.add(tag=\"{}Loss\".format(prefix), simple_value=loss_val_mean)\n",
    "    error_summary.value.add(tag=\"{}Error\".format(prefix), simple_value=error_val_mean)\n",
    "\n",
    "    # Add it to the Tensorboard summary writer\n",
    "    # Make sure to specify a step parameter to get nice graphs over time\n",
    "    summary_writer.add_summary(loss_summary, epoch)\n",
    "    summary_writer.add_summary(error_summary, epoch)\n",
    "\n",
    "    return loss_val_mean, error_val_mean, prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for saving the best net coefficients and stopping early if the loss val is not improving\n",
    "def early_stopping(loss_val,epoch,stop_epochs):\n",
    "    \"\"\"\n",
    "    Save the model coefficients if the data loss function value is better than the last\n",
    "    loss function value.\n",
    "    Return the epoch at which the best loss was and the value of the loss (ergo at which the last checkpoint was created)\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_training = False\n",
    "    \n",
    "    # initialize function attributes\n",
    "    if not hasattr(early_stopping,\"best_loss\"):\n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.best_epoch = epoch\n",
    "    \n",
    "    # if loss val is better than best_loss save the model parameters\n",
    "    if loss_val < early_stopping.best_loss:\n",
    "        saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow') \n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.best_epoch = epoch\n",
    "        print(\"Model saved at epoch {} \\nwith testing loss: {}.\".format(epoch,loss_val))\n",
    "    else:\n",
    "        print(\"Model NOT saved at epoch {} \\nwith testing loss: {}\".format(epoch,loss_val))\n",
    "    \n",
    "    # if the loss didn't improve for the last stop_epochs number of epochs then the training process will stop\n",
    "    if (epoch - early_stopping.best_epoch) >= stop_epochs:\n",
    "        stop_training = True              \n",
    "    \n",
    "    return early_stopping.best_loss, early_stopping.best_epoch, stop_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING THE NETWORK\n",
    "\n",
    "# number of iterations in each epoch\n",
    "# n_iter = int(np.rint((len(x_train)-par.delay-par.pred_step)//(par.batch_size*par.pred_step+par.delay))) # round to nearest integer\n",
    "n_iter = (len(x_train)-par.delay)//(par.batch_size*par.pred_step)\n",
    "n_iter_test = (len(x_test)-par.delay)//(par.batch_size*par.pred_step)\n",
    "n_iter_validation = (len(x_validation)-par.delay)//(par.batch_size*par.pred_step)\n",
    "\n",
    "# initializer of TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # initialize helping variables\n",
    "    stop_training = False\n",
    "    best_epoch = par.n_epochs\n",
    "    \n",
    "    # Restore variables from disk if continueLearning is True.\n",
    "    if continueLearning:\n",
    "        # restoring variables will also initialize them\n",
    "        saver.restore(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "        print(\"Model restored.\")\n",
    "    else:\n",
    "        session.run(init) # initialize variables\n",
    "\n",
    "    \n",
    "    # Create a SummaryWriter to output summaries and the Graph\n",
    "    # in console run 'tensorboard --logdir=./logs/'\n",
    "    summary_writer = tf.summary.FileWriter(logdir=logdir, graph=session.graph)\n",
    "    \n",
    "    for epoch in range(par.n_epochs):\n",
    "        # TRAINING\n",
    "        loss_val, error_val, _ = run_model(x_train,y_train,n_iter,save=False,train=True)\n",
    "        \n",
    "        # TESTING\n",
    "        loss_val_test, error_val_test, _ = run_model(x_test,y_test,n_iter_test,save=False,train=False)\n",
    "        \n",
    "        # write the summaries of testing data at epoch in TensorBoard\n",
    "#        summary_writer.add_summary(summary_test, epoch)\n",
    "            \n",
    "        # increment global step for decaying learning rate at each epoch\n",
    "        session.run(increment_global_step_op)\n",
    "\n",
    "        # Printing the results at every \"par.n_epochs//10\" epochs\n",
    "        if epoch % (par.n_epochs//10) == 0:\n",
    "            print(\"Epoch: {}\".format(epoch))\n",
    "            print(\"TRAINING Loss: {}\".format(loss_val))\n",
    "            print(\"TRAINING Error: {}\".format(error_val))\n",
    "            print(\"TESTING Loss: {}\".format(loss_val_test))\n",
    "            print(\"TESTING Error: {}\".format(error_val_test))\n",
    "            # flush the summary data into TensorBoard\n",
    "            # summary_writer.flush()\n",
    "            \n",
    "        # Checking the model loss_value for early stopping each 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            # save the trained net and variables for later use if the test loss_val is better than the last saved one\n",
    "            best_loss, best_epoch, stop_training = early_stopping(loss_val_test,epoch,par.stop_epochs)\n",
    "            print(\"____________________________\")\n",
    "            \n",
    "        # Stop the training process\n",
    "        if stop_training:\n",
    "            print(\"The training process stopped prematurely at epoch {}.\".format(epoch))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restoring the model coefficients with best results\n",
    "with tf.Session() as session:\n",
    "        \n",
    "    # restore the net coefficients with the lowest loss value\n",
    "    saver.restore(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "    print('Restored model coefficients at epoch {} with TESTING loss val: {:.4f}'.format(best_epoch, best_loss))\n",
    "        \n",
    "    # run the trained net with best coefficients on all time-series and save the results\n",
    "    loss_val, error_val, prediction_list = run_model(x_train,y_train,n_iter,save=True,train=False)\n",
    "    loss_val_test, error_val_test, prediction_list_test = run_model(x_test,y_test,n_iter_test,save=True,train=False)\n",
    "    loss_val_validation, error_val_validation, prediction_list_validation = run_model(x_validation,y_validation,n_iter_validation,save=True,train=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing parameters and results to console\n",
    "results = '''Timestamp: {}\n",
    "_____________________________________________\n",
    "Net parameters:\n",
    "{}\n",
    "_____________________________________________\n",
    "Results: \\n\n",
    "Best epoch ...................... {:4d}\n",
    "TRAINING Loss ................... {:11.6f}\n",
    "TRAINING Error .................. {:11.6f}\n",
    "TESTING Loss .................... {:11.6f}\n",
    "TESTING Error ................... {:11.6f}\n",
    "VALIDATION Loss ................. {:11.6f}\n",
    "VALIDATION Error ................ {:11.6f}\n",
    "_____________________________________________'''.format(now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                                                       ,par\n",
    "                                                       ,best_epoch\n",
    "                                                       ,loss_val\n",
    "                                                       ,error_val\n",
    "                                                       ,loss_val_test\n",
    "                                                       ,error_val_test\n",
    "                                                       ,loss_val_validation\n",
    "                                                       ,error_val_validation)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving results to file\n",
    "file = \"./logs/multi-step/{}/log.txt\".format(now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "with open(file, mode='w') as f:\n",
    "    f.write(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the predictions \"par.delay\" time-steps to the right\n",
    "prediction_train = np.array(prediction_list)\n",
    "prediction_test = np.array(prediction_list_test)\n",
    "prediction_validation = np.array(prediction_list_validation)\n",
    "\n",
    "print(np.shape(prediction_train))\n",
    "\n",
    "prediction_train = np.pad(prediction_train,pad_width=((par.delay,0))\n",
    "                          ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "prediction_test = np.pad(prediction_test,pad_width=((par.delay,0))\n",
    "                         ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "prediction_validation = np.pad(prediction_validation,pad_width=((par.delay,0))\n",
    "                         ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "\n",
    "print(np.shape(prediction_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Denormalize target values\n",
    "def denormalize(labels):\n",
    "    # denormalize the labels\n",
    "    # denormalized_labels = min_data + labels*(max_data - min_data)\n",
    "    # denormalize the labels from (-1,1)\n",
    "    denormalized_labels = mean_data + labels*std_data\n",
    "    \n",
    "    return denormalized_labels\n",
    "\n",
    "\n",
    "y_train_denorm = denormalize(y_train)\n",
    "y_test_denorm = denormalize(y_test)\n",
    "y_validation_denorm = denormalize(y_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "def plot_results(predictions, targets):\n",
    "    plt.plot(predictions)\n",
    "    plt.plot(targets, alpha=0.6)\n",
    "    plt.xlabel('time (weeks / days)')\n",
    "    plt.ylabel('flow rate (unknown)')\n",
    "    plt.legend(['predictions', 'targets'])\n",
    "    plt.draw()\n",
    "\n",
    "plot_start = int(days_in_year*3)\n",
    "plot_end = int(days_in_year*5)\n",
    "    \n",
    "f_training = plt.figure()\n",
    "plot_results(prediction_train[plot_start:plot_end], y_train_denorm[plot_start:plot_end])\n",
    "plt.title('TRAINING')\n",
    "\n",
    "f_testing = plt.figure()\n",
    "plot_results(prediction_test[plot_start:plot_end], y_test_denorm[plot_start:plot_end])\n",
    "plt.title('TESTING')\n",
    "\n",
    "f_validation = plt.figure()\n",
    "plot_results(prediction_validation[plot_start:plot_end], y_validation_denorm[plot_start:plot_end])\n",
    "plt.title('VALIDATION')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the figures:\n",
    "img_save_dir = \"IMG\\\\\" + now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "save_dir_path = os.path.join(os.curdir, img_save_dir)\n",
    "os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "f_training.savefig(save_dir_path + \"\\\\training.pdf\", bbox_inches='tight')\n",
    "f_testing.savefig(save_dir_path + \"\\\\testing.pdf\", bbox_inches='tight')\n",
    "f_validation.savefig(save_dir_path + \"\\\\validation.pdf\", bbox_inches='tight')\n",
    "\n",
    "print(\"Figures saved to: {}\".format(save_dir_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following cells are for debugging purposes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_batch, y_batch = create_batch(x_train,y_train,par.batch_size,1)\n",
    "\n",
    "print(np.shape(x_batch),np.shape(y_batch))\n",
    "print(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    " - Variable length last batch to make use of all the available data!!!\n",
    " - plot every par.n_epochs//10 into tensorboard (different charts?)\n",
    " - try chaning par.hidden_size --- DONE\n",
    " - try chaning par.delay\n",
    " - try chaning par.n_lstm_layers\n",
    " - regularization (not necessarry)\n",
    "\n",
    "\n",
    " - test and validation --- DONE\n",
    " - drift??? --- DONE\n",
    " - Show the graph in TensorBoard -- DONE\n",
    " - add Dropout layer -- DONE\n",
    " - divide loss sum by n_iter to make it a loss val mean !!! --- DONE\n",
    " - add relative error between the predictions and targets --- DONE\n",
    "\n",
    "#### Hyperparameter tuning\n",
    " - smaller batch size is better because you have to predict less things before updating weights\n",
    "   but makes the training values more unstable\n",
    " - more prediction steps are worse\n",
    " - bigger delay?\n",
    " - number of stacked layers?\n",
    " - smaller hidden size => faster, easier learning, less overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayed_learning_rate_test = 0.01 * 0.90 ** (2000 / 1000)\n",
    "print(decayed_learning_rate_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# function for generating the time-series batches\n",
    "def create_batch(x_data,y_data,par.batch_size,par.delay,par.input_size,index):\n",
    "    \n",
    "    x_batch = np.zeros([par.batch_size,par.delay,par.input_size])\n",
    "    x_pad = np.zeros([par.batch_size,par.pred_step-1,par.input_size])\n",
    "    y_batch = np.zeros([par.batch_size,par.num_classes,par.pred_step])\n",
    "    \n",
    "    step = index*(par.batch_size*par.pred_step)\n",
    "    \n",
    "    for i in range(par.batch_size):\n",
    "        x_batch[i,:,:] = np.reshape(x_data[step+i*par.pred_step:step+i*par.pred_step+par.delay],(par.delay,par.num_classes))\n",
    "        y_batch[i,:] = np.reshape(y_data[step+par.delay+i*par.pred_step+1:step+par.delay+i*par.pred_step+1+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "        \n",
    "    x_batch = np.hstack((x_batch, x_pad))\n",
    "        \n",
    "    return x_batch, y_batch\n",
    "\n",
    "x_batch, y_batch = create_batch(x_train,y_train,2,4,par.input_size,1)\n",
    "\n",
    "print(x_batch)\n",
    "print('______')\n",
    "print(y_batch)\n",
    "\n",
    "print('\\n ___batch 2___ \\n')\n",
    "x_batch, y_batch = create_batch(x_train,y_train,2,4,par.input_size,2)\n",
    "\n",
    "print(x_batch)\n",
    "print('______')\n",
    "print(y_batch)\n",
    "\n",
    "print('\\n ___batch 3___ \\n')\n",
    "x_batch, y_batch = create_batch(x_train,y_train,2,4,par.input_size,3)\n",
    "\n",
    "print(x_batch)\n",
    "print('______')\n",
    "print(y_batch)\n",
    "\n",
    "# FIXED: with every batch we jumped over one value while generating batches"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# list of np arrays\n",
    "vectors = [np.array([1, 2, 3])]*5\n",
    "\n",
    "print(vectors)\n",
    "\n",
    "np_vectors = np.array(vectors)\n",
    "\n",
    "print(np.shape(np_vectors))\n",
    "\n",
    "np_vectors = np.reshape(np_vectors,(1,3*5))\n",
    "print(np_vectors)\n",
    "print(np.shape(np_vectors))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(x_train)\n",
    "#print(n_iter)\n",
    "print(14)\n",
    "print((len(x_train)-par.delay)//(par.batch_size*par.pred_step))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.mean(x_train)\n",
    "print(np.shape(x_test), np.shape(x_test[:4000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print only tensor v1 in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file('./checkpoints/Multi-Step_LSTMforPredictingLabeFlow',\n",
    "                                      tensor_name='output_layer/weights_out', \n",
    "                                      all_tensors=False, \n",
    "                                      all_tensor_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_val = [1,2,3,4]\n",
    "\n",
    "pred_list = list(list_val[-i-1] for i in range(par.pred_step))\n",
    "pred_list = pred_list[::-1]\n",
    "\n",
    "print(pred_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Early stopping\n",
    "def early_stopping(loss_val,epoch):\n",
    "    \"\"\"\n",
    "    Save the model coefficients if the data loss function value is better than the last\n",
    "    loss function value.\n",
    "    Return the epoch at which the best loss was and the value of the loss (ergo at which the last checkpoint was created)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize function attributes\n",
    "    if not hasattr(early_stopping,\"best_loss\"):\n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.epoch = 0\n",
    "    \n",
    "    # compare the loss_val\n",
    "    if loss_val < early_stopping.best_loss:\n",
    " #       saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow') \n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.epoch = epoch\n",
    "        \n",
    "    return early_stopping.best_loss, early_stopping.epoch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(early_stopping(0.5,0))\n",
    "print(early_stopping(0.6,10))\n",
    "print(early_stopping(0.4,20))\n",
    "print(early_stopping(0.8,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define net parameters\n",
    "class params:\n",
    "    # initialization of instance variables\n",
    "    def __init__(self,n_lstm_layers,hidden_size,delay,pred_step,batch_size,n_epochs,stop_epochs,init_lr):\n",
    "        self.input_size = 1 # number of input features (we only follow one variable == flow)\n",
    "        self.num_classes = 1 # number of output classes (we wan't specific value, not classes, so this is 1)\n",
    "        self.target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "        self.n_lstm_layers = n_lstm_layers # number of vertically stacked LSTM layers\n",
    "        self.hidden_size = hidden_size # hidden state vector size in LSTM cell\n",
    "        self.delay = delay # the number of time-steps from which we are going to predict the next step\n",
    "        self.pred_step = pred_step # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "        self.batch_size = batch_size # number of inputs in one batch\n",
    "        self.n_epochs = n_epochs # number of epochs\n",
    "        self.stop_epochs = stop_epochs # if the loss value doesn't improve over the last stop_epochs, the training process will stop\n",
    "        self.init_lr = init_lr # initial learning rate for Adam optimizer (training algorithm)\n",
    "        self.net_unroll_size = delay + pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "        \n",
    "    # how will the class object be represented in string form (eg. when called with print())\n",
    "    def __str__(self):\n",
    "        answer = '''\n",
    "Input size ...................... {:4d}\n",
    "Number of classes ............... {:4d}\n",
    "Target shift .................... {:4d}\n",
    "Number of stacked LSTM layers ... {:4d}\n",
    "Hidden state size ............... {:4d}\n",
    "Delay ........................... {:4d}\n",
    "Number of prediciton steps....... {:4d}\n",
    "Batch size ...................... {:4d}\n",
    "Maximum number of epochs ........ {:4d}\n",
    "Early stopping epochs ........... {:4d}\n",
    "Initial learning rate ........... {:9.4f}'''.format(self.input_size\n",
    "                                              ,self.num_classes\n",
    "                                              ,self.target_shift\n",
    "                                              ,self.n_lstm_layers\n",
    "                                              ,self.hidden_size\n",
    "                                              ,self.delay\n",
    "                                              ,self.pred_step\n",
    "                                              ,self.batch_size\n",
    "                                              ,self.n_epochs\n",
    "                                              ,self.stop_epochs\n",
    "                                              ,self.init_lr)\n",
    "        return answer\n",
    "        # return str(vars(self))\n",
    "\n",
    "# net and training parameter specification\n",
    "par = params(2,20,10,1,5,100,20,0.001)\n",
    "\n",
    "print(par)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p1 = params()\n",
    "p2 = params()\n",
    "p1.par.batch_size = 10\n",
    "p2.par.batch_size = 20\n",
    "\n",
    "print(p1.par.batch_size, p2.par.batch_size)\n",
    "\n",
    "p1.par.delay = 5\n",
    "\n",
    "print(p1.par.delay)\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variables = vars(p1)\n",
    "#print(variables)\n",
    "print(\"Params: {}\".format(p1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p1.par.input_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
