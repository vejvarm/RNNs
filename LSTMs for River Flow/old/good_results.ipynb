{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define unique name for log directory\n",
    "now = datetime.now()\n",
    "logdir = \"./logs/multi-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "# define constants\n",
    "input_size = 1\n",
    "batch_size = 20\n",
    "n_lstm_layers = 4 # number of vertically stacked LSTM layers\n",
    "num_units = 64 # hidden state vector size in LSTM cell\n",
    "num_classes = 1 # output size (only one specific value)\n",
    "target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "delay = 100 # the number of time-steps from which we are going to predict the next step\n",
    "pred_step = 3 # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "net_unroll_size = delay + pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "n_epochs = 40 # number of epochs\n",
    "\n",
    "# FIGHTING OVERFITTING:\n",
    "# enable/disable input and output dropout in LSTM layer\n",
    "dropout = False\n",
    "# enable/disable L2 regularization\n",
    "regularization = False # NOT IMPLEMENTED YET (go to: https://stackoverflow.com/questions/37869744/tensorflow-lstm-regularization)\n",
    "\n",
    "# continue learning the model with saved variables from previeous training\n",
    "continueLearning = False\n",
    "\n",
    "# decaying learning rate constants (for exponential decay)\n",
    "initial_learning_rate = 0.001 # initial learning rate for Adam training algorithm\n",
    "decay_steps = n_epochs//10 # every \"n_epochs//10\" epochs the learning rate is reduced\n",
    "decay_rate = 1 # the base of the exponential (rate of the decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# BEST SO FAR!\n",
    "# Testing Error: 0.10290688012774851\n",
    "# Validation Error: 0.23906627175812084\n",
    "# logs: 20180417-004046\n",
    "\n",
    "# define unique name for log directory\n",
    "now = datetime.now()\n",
    "logdir = \"./logs/multi-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "# define constants\n",
    "input_size = 1\n",
    "batch_size = 3\n",
    "n_lstm_layers = 4 # number of vertically stacked LSTM layers\n",
    "num_units = 64 # hidden state vector size in LSTM cell\n",
    "num_classes = 1 # output size (only one specific value)\n",
    "target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "delay = 100 # the number of time-steps from which we are going to predict the next step\n",
    "pred_step = 3 # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "net_unroll_size = delay + pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "n_epochs = 20 # number of epochs\n",
    "\n",
    "# FIGHTING OVERFITTING:\n",
    "# enable/disable input and output dropout in LSTM layer\n",
    "dropout = False\n",
    "# enable/disable L2 regularization\n",
    "regularization = False # NOT IMPLEMENTED YET (go to: https://stackoverflow.com/questions/37869744/tensorflow-lstm-regularization)\n",
    "\n",
    "# continue learning the model with saved variables from previeous training\n",
    "continueLearning = False\n",
    "\n",
    "# decaying learning rate constants (for exponential decay)\n",
    "initial_learning_rate = 0.001 # initial learning rate for Adam training algorithm\n",
    "decay_steps = n_epochs//10 # every \"n_epochs//10\" epochs the learning rate is reduced\n",
    "decay_rate = 1 # the base of the exponential (rate of the decay)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
