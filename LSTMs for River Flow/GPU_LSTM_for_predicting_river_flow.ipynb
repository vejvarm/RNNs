{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of river flow using RNN with LSTM architecture"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install scipy openpyxl matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-da79304c563c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# from tensorflow.nn import dynamic_rnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minspect_checkpoint\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mchkp\u001b[0m \u001b[1;31m# import the inspect_checkpoint library\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# import tensorboard as tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfactorization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\distributions\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msoftplus_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribution_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtridiag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeometric\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhalf_normal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\distributions\\python\\ops\\estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_compute_weighted_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_RegressionHead\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbasic_session_run_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mestimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\basic_session_run_hooks.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[1;34m'tf.contrib.learn.basic_session_run_hooks.LoggingTensorHook'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;34m'tf.train.LoggingTensorHook'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     basic_session_run_hooks.LoggingTensorHook)\n\u001b[0m\u001b[0;32m     34\u001b[0m StopAtStepHook = deprecated_alias(\n\u001b[0;32m     35\u001b[0m     \u001b[1;34m'tf.contrib.learn.basic_session_run_hooks.StopAtStepHook'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mdeprecated_alias\u001b[1;34m(deprecated_name, name, func_or_class, warn_once)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;31m# Make a new class with __init__ wrapped in a warning.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     \u001b[1;32mclass\u001b[0m \u001b[0mNewClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m       __doc__ = decorator_utils.add_notice_to_docstring(\n\u001b[0;32m    149\u001b[0m           \u001b[0mfunc_or_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Please use %s instead.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mNewClass\u001b[1;34m()\u001b[0m\n\u001b[0;32m    152\u001b[0m                            'It will be removed in a future version. '])\n\u001b[0;32m    153\u001b[0m       \u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_or_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m       \u001b[0m__module__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36m_call_location\u001b[1;34m(outer)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m   \u001b[1;34m\"\"\"Returns call location given level up from current call.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m   \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# CPython internals are available, use them for performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m   \u001b[1;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(context)\u001b[0m\n\u001b[0;32m   1492\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1493\u001b[0m     \u001b[1;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1494\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1470\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1471\u001b[1;33m         \u001b[0mframeinfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1472\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1473\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[1;34m(frame, context)\u001b[0m\n\u001b[0;32m   1443\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1444\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1445\u001b[1;33m             \u001b[0mlines\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1446\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'source code not available'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m     \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinecache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetmodule\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    737\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0m_filesbymodname\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 739\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetabsfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    740\u001b[0m             \u001b[1;31m# Always map to the name the module knows itself by\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m             modulesbyfile[f] = modulesbyfile[\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetabsfile\u001b[1;34m(object, _filename)\u001b[0m\n\u001b[0;32m    706\u001b[0m     normalizes the result as much as possible.\"\"\"\n\u001b[0;32m    707\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_filename\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 708\u001b[1;33m         \u001b[0m_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormcase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[1;34m(object)\u001b[0m\n\u001b[0;32m    691\u001b[0m                  importlib.machinery.EXTENSION_SUFFIXES):\n\u001b[0;32m    692\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     \u001b[1;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "# from tensorflow.nn import dynamic_rnn\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp # import the inspect_checkpoint library\n",
    "# import tensorboard as tb\n",
    "import numpy as np\n",
    "import scipy.io as sio # for working with .mat files\n",
    "# import pandas as pd # for working with .xlsx files\n",
    "from openpyxl import load_workbook # for working with .xlsx files\n",
    "import matplotlib.pyplot as plt # for plotting the data\n",
    "from datetime import datetime # for keeping separate TB logs for each run\n",
    "import os, sys\n",
    "import textwrap\n",
    "import time # for measuring time cost\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NAMED TUPLE CLASS ALTERNATIVE FOR DEFINING PARAMS\n",
    "fields = ['input_size',\n",
    "          'num_classes',\n",
    "          'target_shift',\n",
    "          'n_lstm_layers',\n",
    "          'hidden_size',\n",
    "          'delay',\n",
    "          'pred_step',\n",
    "          'batch_size', # tuple of 3 values (train,test,validation)\n",
    "          'n_epochs',\n",
    "          'stop_epochs',\n",
    "          'check_every',\n",
    "          'init_lr',\n",
    "          'n_repeats',\n",
    "          'net_unroll_size',\n",
    "          'dropout',\n",
    "          'keep_prob']  # tuple of 3 values (input,output,recurrent)\n",
    "params = namedtuple('params',fields)\n",
    "\n",
    "par = params(input_size=1, num_classes=1, target_shift=1,\n",
    "             n_lstm_layers=2, hidden_size=10, delay=10,\n",
    "             pred_step=1, batch_size=(5,5,5), n_epochs=100,\n",
    "             stop_epochs=20, check_every=10, init_lr=0.001,\n",
    "             n_repeats=1, net_unroll_size=10, dropout=False,\n",
    "             keep_prob=(1.0,1.0,1.0))\n",
    "\n",
    "# CREATING ORDERED DICTIONARY FROM THE TUPLE AND PRINTING IT'S ITMES WITH KEYS\n",
    "for key, value in par._asdict().items():\n",
    "    print(key + ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net parameters\n",
    "class params:\n",
    "    # initialization of instance variables\n",
    "    def __init__(self,n_lstm_layers=2,hidden_size=10,delay=10,pred_step=1,train_batch_size=5,test_batch_size=5,val_batch_size=5,n_epochs=100,stop_epochs=20,check_every=10,init_lr=0.001,n_repeats=1,dropout=False,input_keepProb=1,output_keepProb=1,recurrent_keepProb=1):\n",
    "        self.input_size = 1 # number of input features (we only follow one variable == flow)\n",
    "        self.num_classes = 1 # number of output classes (we wan't specific value, not classes, so this is 1)\n",
    "        self.target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "        self.n_lstm_layers = n_lstm_layers # number of vertically stacked LSTM layers\n",
    "        self.hidden_size = hidden_size # hidden state vector size in LSTM cell\n",
    "        self.delay = delay # the number of time-steps from which we are going to predict the next step\n",
    "        self.pred_step = pred_step # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "        self.train_batch_size = train_batch_size # number of inputs in one training batch\n",
    "        self.test_batch_size = test_batch_size # number of inputs in one testing batch\n",
    "        self.val_batch_size = val_batch_size # number of inputs in one validation batch\n",
    "        self.n_epochs = n_epochs # number of epochs\n",
    "        self.stop_epochs = stop_epochs # if the loss value doesn't improve over the last stop_epochs, the training process will stop\n",
    "        self.check_every = check_every # how often to check for loss value in early stopping\n",
    "        self.init_lr = init_lr # initial learning rate for Adam optimizer (training algorithm)\n",
    "        self.n_repeats = n_repeats # number of repeats of the whole training and validation process with the same params\n",
    "        self.net_unroll_size = self.delay + self.pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "        # FIGHTING OVERFITTING:\n",
    "        self.dropout = dropout # if true the dropout is applied on inputs, outputs and recurrent states of cells\n",
    "        self.input_keepProb = input_keepProb # (dropout) probability of keeping the input\n",
    "        self.output_keepProb = output_keepProb # (dropout) probability of keeping the output\n",
    "        self.recurrent_keepProb = recurrent_keepProb # (dropout) probability of keeping the recurrent state\n",
    "    \n",
    "    # representation of object for interpreter and debugging purposes\n",
    "    def __repr__(self):\n",
    "        return '''params(n_lstm_layers={:d},hidden_size={:d},delay={:d},pred_step={:d},\\\n",
    "train_batch_size={:d},test_batch_size={:d},val_batch_size={:d},n_epochs={:d},stop_epochs={:d},check_every={:d},init_lr={:f},dropout={},\\\n",
    "n_repeats={:d},input_keepProb={:f},output_keepProb={:f},recurrent_keepProb={:f})'''.format(self.n_lstm_layers,\n",
    "                                                                            self.hidden_size,\n",
    "                                                                            self.delay,\n",
    "                                                                            self.pred_step,\n",
    "                                                                            self.train_batch_size,\n",
    "                                                                            self.test_batch_size,\n",
    "                                                                            self.val_batch_size,\n",
    "                                                                            self.n_epochs,\n",
    "                                                                            self.stop_epochs,\n",
    "                                                                            self.check_every,\n",
    "                                                                            self.init_lr,\n",
    "                                                                            self.n_repeats,\n",
    "                                                                            self.dropout,\n",
    "                                                                            self.input_keepProb,\n",
    "                                                                            self.output_keepProb,\n",
    "                                                                            self.recurrent_keepProb)\n",
    "    \n",
    "    # how will the class object be represented in string form (eg. when called with print())\n",
    "    def __str__(self):\n",
    "        answer = '''\n",
    "Input size ...................... {:4d}\n",
    "Number of classes ............... {:4d}\n",
    "Target shift .................... {:4d}\n",
    "Number of stacked LSTM layers ... {:4d}\n",
    "Hidden state size ............... {:4d}\n",
    "Delay ........................... {:4d}\n",
    "Number of prediciton steps....... {:4d}\n",
    "Training batch size ............. {:4d}\n",
    "Testing batch size .............. {:4d}\n",
    "Validation batch size ........... {:4d}\n",
    "Maximum number of epochs ........ {:4d}\n",
    "Early stopping epochs ........... {:4d}\n",
    "Check (save) every (epochs)...... {:4d}\n",
    "Initial learning rate ........... {:9.4f}\n",
    "Dropout ......................... {}'''.format(self.input_size\n",
    "                                              ,self.num_classes\n",
    "                                              ,self.target_shift\n",
    "                                              ,self.n_lstm_layers\n",
    "                                              ,self.hidden_size\n",
    "                                              ,self.delay\n",
    "                                              ,self.pred_step\n",
    "                                              ,self.train_batch_size\n",
    "                                              ,self.test_batch_size\n",
    "                                              ,self.val_batch_size\n",
    "                                              ,self.n_epochs\n",
    "                                              ,self.stop_epochs\n",
    "                                              ,self.check_every\n",
    "                                              ,self.init_lr\n",
    "                                              ,self.dropout)\n",
    "        \n",
    "        dropout_answer = '''\n",
    "Input keep probability .......... {:7.2f}\n",
    "Output keep probability ......... {:7.2f}\n",
    "Recurrent keep probability ...... {:7.2f}'''.format(self.input_keepProb\n",
    "                                                 ,self.output_keepProb\n",
    "                                                 ,self.recurrent_keepProb)\n",
    "        \n",
    "        if self.dropout:\n",
    "            return answer + dropout_answer\n",
    "        else:\n",
    "            return answer\n",
    "\n",
    "# net and training parameter specification\n",
    "par = params(n_lstm_layers = 2\n",
    "            ,hidden_size = 16\n",
    "            ,delay = 32\n",
    "            ,pred_step = 1\n",
    "            ,train_batch_size=16 # (https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/)\n",
    "            ,test_batch_size=4\n",
    "            ,val_batch_size=4\n",
    "            ,n_epochs=1000\n",
    "            ,stop_epochs=50 # if <= 0 then early stopping is disabled and check every sets the saving period\n",
    "            ,check_every=10\n",
    "            ,init_lr=0.001\n",
    "            ,n_repeats=1\n",
    "            ,dropout=False\n",
    "            ,input_keepProb=1.0\n",
    "            ,output_keepProb=1.0\n",
    "            ,recurrent_keepProb=1.0)\n",
    "    \n",
    "\n",
    "# enable/disable L2 regularization\n",
    "regularization = False # NOT IMPLEMENTED YET (go to: https://stackoverflow.com/questions/37869744/tensorflow-lstm-regularization)\n",
    "\n",
    "# continue training the model with saved variables from previeous training\n",
    "continueLearning = False\n",
    "\n",
    "# decaying learning rate constants (for exponential decay)\n",
    "# Adam already has adaptive learning rate for individual weights \n",
    "# but it can be combined with decaying learning rate for faster convergence\n",
    "decay_steps = par.n_epochs//10 # every \"n_epochs//10\" epochs the learning rate is reduced\n",
    "decay_rate = 1 # the base of the exponential (rate of the decay ... 1 == no decay)\n",
    "\n",
    "# deciding which device (CPU/GPU) should be used for tf operations\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    device = \"/gpu:0\"\n",
    "else:\n",
    "    device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select input data for the network\n",
    "# 1) ... Weekly Elbe flow (normalized)\n",
    "# 2) ... Daily Saugeen flow (unknown)\n",
    "selected_data = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) DATA from prof. A. Procházka:\n",
    "* **url:**: http://uprt.vscht.cz/prochazka/pedag/Data/dataNN.zip\n",
    "* **name**: Weekly Elbe river flow\n",
    "* **Provider source:** Prof. Ing. Aleš Procházka, CSc\n",
    "* **Span:** 313 weeks ~ 6 years of data\n",
    "* **Data size:** 313 values\n",
    "* **Already normalized** to 0 mean and 1 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_data == 1:\n",
    "    # load data from Q.mat\n",
    "    filename = './datasets/Q.MAT'\n",
    "    data = sio.loadmat(filename) # samples were gathered with period of one week\n",
    "\n",
    "    # convert to np array\n",
    "    data = np.array(data['Q'],dtype=np.float32)\n",
    "\n",
    "    print(np.shape(data))\n",
    "\n",
    "    # normalize the data to interval (0,1)\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "    # shift the data to start at 0.001 (for relative error counting purposes)\n",
    "#    data = np.subtract(data,min_data)+0.001\n",
    "    # data = np.divide(np.subtract(data,min_data),np.subtract(max_data,min_data)).flatten()\n",
    "    # normalize the data to interval (-1,1) (cca 0 mean and 1 variance)\n",
    "    mean_data = np.mean(data) # mean\n",
    "    std_data = np.std(data) # standard deviation\n",
    "    data = np.divide(np.subtract(data,mean_data),std_data).flatten()\n",
    "\n",
    "    # divide the data into training, testing and validation part\n",
    "    weeks_in_year = 52.1775\n",
    "    years_in_data = 313/weeks_in_year\n",
    "\n",
    "    years_in_train = int(years_in_data*0.7) # 70% of data rounded to the number of years\n",
    "    years_in_test = int(np.ceil(years_in_data*0.15)) # 15% of data rounded to the number of years\n",
    "\n",
    "    weeks_train = int(years_in_train*weeks_in_year) # number of weeks in training data\n",
    "    weeks_test = int(years_in_test*weeks_in_year) # number of weeks in testing data\n",
    "\n",
    "    end_of_train = weeks_train\n",
    "    end_of_test = weeks_train + weeks_test\n",
    "\n",
    "    x_train = data[:end_of_train]\n",
    "    x_test = data[end_of_train:end_of_test]\n",
    "    x_validation = data[end_of_test:]\n",
    "\n",
    "    plot_start = 0\n",
    "    plot_end = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) DATA from Time Series Data Library:\n",
    "* **url:** https://datamarket.com/data/set/235a/mean-daily-saugeen-river-flows-jan-01-1915-to-dec-31-1979#!ds=235a&display=line\n",
    "* **name:** Mean daily Saugeen River (Canada) flows\n",
    "* **Provider source:** Hipel and McLeod (1994)\n",
    "* **Span:** Jan 01, 1915 to Dec 31, 1979\n",
    "* **Data size:** 23741 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_data == 2:\n",
    "    # load excel spreadsheet with openpyxl:\n",
    "    filename = './datasets/sugeen-river-flows.xlsx'\n",
    "    xl = load_workbook(filename)\n",
    "\n",
    "    # print sheet names:\n",
    "    print(xl.get_sheet_names())\n",
    "\n",
    "    # get sheet:\n",
    "    sheet = xl.get_sheet_by_name('Mean daily saugeen River flows,')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # fill a list with values from cells:\n",
    "    for cell in sheet['B16:B23756']:\n",
    "        data.append(cell[0].value)\n",
    "\n",
    "    # convert list to numpy array and reshape to a column vector\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data,(1,-1))\n",
    "\n",
    "    # normalize the data to interval (0,1) <- DONT!\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "    # data = np.divide(np.subtract(data,min_data),np.subtract(max_data,min_data)).flatten()\n",
    "    # !!! CENTERING data:\n",
    "    # normalize the data to interval (-1,1) (cca 0 mean and 1 variance)\n",
    "    # data = data[0,:120]\n",
    "    mean_data = np.mean(data) # mean\n",
    "    std_data = np.std(data) # standard deviation\n",
    "    data = np.divide(np.subtract(data,mean_data),std_data).flatten()\n",
    "\n",
    "    # divide the data into training, testing and validation part\n",
    "    days_in_data = np.shape(data)[0]\n",
    "    days_in_year = 365.25\n",
    "    years_in_data = days_in_data/days_in_year\n",
    "\n",
    "    years_in_train = int(years_in_data*0.7) # 70% of data rounded to the number of years\n",
    "    years_in_test = int(np.ceil(years_in_data*0.15)) # 15% of data rounded to the number of years\n",
    "\n",
    "    days_train = int(years_in_train*days_in_year) # number of days in training data\n",
    "    days_test = int(years_in_test*days_in_year) # number of days in testing data\n",
    "\n",
    "    end_of_train = days_train\n",
    "    end_of_test = days_train + days_test\n",
    "\n",
    "    x_train = data[:end_of_train]\n",
    "    x_test = data[end_of_train:end_of_test]\n",
    "    x_validation = data[end_of_test:]\n",
    "\n",
    "    plot_start = int(days_in_year*3)\n",
    "    plot_end = int(days_in_year*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the shifted time-series (targets)\n",
    "y_train = np.roll(x_train, par.target_shift)\n",
    "y_test = np.roll(x_test, par.target_shift)\n",
    "y_validation = np.roll(x_validation, par.target_shift)\n",
    "\n",
    "# delete the first elements of the time series that were reintroduced from the end of the timeseries\n",
    "y_train[:par.target_shift] = 0\n",
    "y_test[:par.target_shift] = 0\n",
    "y_validation[:par.target_shift] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset TensorFlow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(device):\n",
    "    # define tensorflow constants\n",
    "    min_of_data = tf.constant(min_data, dtype=tf.float32, name='min_of_data')\n",
    "    max_of_data = tf.constant(max_data, dtype=tf.float32, name='max_of_data')\n",
    "    mean_of_data = tf.constant(mean_data, dtype=tf.float32, name='mean_of_data')\n",
    "    std_of_data = tf.constant(std_data, dtype=tf.float32, name='std_of_data')\n",
    "\n",
    "    # define output weights and biases\n",
    "    with tf.name_scope(\"output_layer\"):\n",
    "        weights_out = tf.Variable(tf.random_normal([par.hidden_size,par.num_classes]),name='weights_out')\n",
    "        bias_out = tf.Variable(tf.random_normal([par.num_classes]),name='biases_out')\n",
    "\n",
    "    # define placeholders for the batches of time-series\n",
    "    x = tf.placeholder(tf.float32,[None, par.net_unroll_size, par.input_size],name='x') # batch of inputs\n",
    "    y = tf.placeholder(tf.float32,[None, par.num_classes, par.pred_step],name='y') # batch of labels\n",
    "\n",
    "    # define placeholders for dropout keep probabilities\n",
    "    input_kP = tf.placeholder(tf.float32,name='input_kP')\n",
    "    output_kP = tf.placeholder(tf.float32,name='output_kP')\n",
    "    recurrent_kP = tf.placeholder(tf.float32,name='recurrent_kP')\n",
    "    \n",
    "    # processing the input tensor from [par.batch_size,par.delay,par.input_size] to \"par.delay\" number of [par.batch_size,par.input_size] tensors\n",
    "    input=tf.unstack(x, par.net_unroll_size, 1, name='LSTM_input_list') # create list of values by unstacking one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an LSTM cell:\n",
    "def make_cell(hidden_size):\n",
    "    return rnn.LSTMCell(hidden_size, state_is_tuple=True, activation=tf.tanh)\n",
    "\n",
    "# define an LSTM network with 'par.n_lstm_layers' layers\n",
    "with tf.device(device):\n",
    "    with tf.name_scope(\"LSTM_layer\"):\n",
    "        lstm_cells = rnn.MultiRNNCell([make_cell(par.hidden_size) for _ in range(par.n_lstm_layers)], state_is_tuple=True)\n",
    "\n",
    "        # add dropout to the inputs and outputs of the LSTM cell (reduces overfitting)\n",
    "        lstm_cells = rnn.DropoutWrapper(lstm_cells, input_keep_prob=input_kP, output_keep_prob=output_kP, state_keep_prob=recurrent_kP)\n",
    "\n",
    "        # create static RNN from lstm_cell\n",
    "        outputs,_ = rnn.static_rnn(lstm_cells, input, dtype=tf.float32)\n",
    "    #    outputs,_ = tf.nn.dynamic_rnn(lstm_cells, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of predictions based on the last \"par.pred_step\" time-step outputs (multi-step prediction)\n",
    "with tf.device(device):\n",
    "    prediction = [tf.matmul(outputs[-i-1],weights_out) + bias_out for i in (range(par.pred_step))] # newest prediction first\n",
    "    prediction = prediction[::-1] #reverse the list (oldest prediction first)\n",
    "\n",
    "    prediction = tf.reshape(prediction,[-1,par.num_classes,par.pred_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function with regularization\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "with tf.device(device):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "    #    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "        loss = tf.sqrt(tf.losses.mean_squared_error(predictions=prediction,labels=y)) # RMSE (root mean squared error)\n",
    "\n",
    "# exponential decay of learning rate with epochs\n",
    "global_step = tf.Variable(1, trainable=False, name='global_step') # variable that keeps track of the step at which we are in the training\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1,name='increment_global_step') # operation that increments global step by one\n",
    "\n",
    "with tf.device(device):\n",
    "    # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "    learning_rate = tf.train.exponential_decay(par.init_lr, global_step,\n",
    "                                               decay_steps, decay_rate, staircase=True) # decay at discrete intervals\n",
    "\n",
    "    # define Adam optimizer for training of the network\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(device):\n",
    "    # denormalize data (from (0,1)):\n",
    "    # denormalized_prediction = min_of_data + tf.multiply(prediction, tf.subtract(max_of_data,min_of_data))\n",
    "    # denormalized_y = min_of_data + tf.multiply(y, tf.subtract(max_of_data,min_of_data))\n",
    "\n",
    "    # denormalize data (from (-1,1)): <- better\n",
    "    denormalized_prediction = mean_of_data + tf.multiply(prediction, std_of_data)\n",
    "    denormalized_y = mean_of_data + tf.multiply(y, std_of_data)\n",
    "\n",
    "    # calculate relative error of denormalized data:\n",
    "    with tf.name_scope(\"SMAPE\"):\n",
    "        SMAPE = 2*tf.reduce_mean(tf.divide(tf.abs(tf.subtract(denormalized_prediction,denormalized_y))\n",
    "                                           ,tf.add(tf.abs(denormalized_y),tf.abs(denormalized_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard summaries (visualization logs)\n",
    "# histogram summary for output weights\n",
    "w_out_summ = tf.summary.histogram(\"w_out_summary\", weights_out)\n",
    "# scalar summary for loss function\n",
    "# training_loss_summ = tf.summary.scalar(\"training_loss\",loss)\n",
    "# testing_loss_summ = tf.summary.scalar(\"testing_loss\",loss)\n",
    "# scalar summary for relative error\n",
    "# training_error_summ = tf.summary.scalar(\"relative_training_error\",SMAPE)\n",
    "# testing_error_summ = tf.summary.scalar(\"relative_testing_error\",SMAPE)\n",
    "\n",
    "# NOT USEFUL HERE\n",
    "# merge the summaries of all tf.summary calls (for TensorBoard visualization)\n",
    "# merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(x_data,y_data,batch_size,index):\n",
    "    \"\"\"\n",
    "    function for generating the time-series batches\n",
    "\n",
    "    :param x_data: input data series\n",
    "    :param y_data: output data series\n",
    "    :param batch_size: size of one batch of data to feed into network\n",
    "    :param index: index of the current batch of data\n",
    "    :return: input and output batches of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_batch = np.zeros([batch_size,par.delay,par.input_size])\n",
    "    x_pad = np.zeros([batch_size,par.pred_step-1,par.input_size])\n",
    "    y_batch = np.zeros([batch_size,par.num_classes,par.pred_step])\n",
    "    \n",
    "    step = index*(batch_size*par.pred_step)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_batch[i,:,:] = np.reshape(x_data[step+i*par.pred_step:step+i*par.pred_step+par.delay],(par.delay,par.num_classes))\n",
    "        y_batch[i,:] = np.reshape(y_data[step+par.delay+i*par.pred_step+1:step+par.delay+i*par.pred_step+1+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "    \n",
    "    # the last \"par.pred_step - 1\" columns in x_batch are padded with 0\n",
    "    # because there are no inputs into the net at these time steps\n",
    "    x_batch = np.hstack((x_batch, x_pad))\n",
    "    \n",
    "#    print(x_batch)\n",
    "#    print('________________')\n",
    "#    print(y_batch)\n",
    "#    print('================')\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(inputs,labels,batch_size,save=False,train=False):\n",
    "    \"\"\"\n",
    "    feeding the model with batches of inputs, running the optimizer for training and getting training and testing results\n",
    "\n",
    "    :param inputs: input data series\n",
    "    :param labels: output data series\n",
    "    :param batch_size: the size of the data batch\n",
    "    :param save: if True the predicted time series is returned as a list\n",
    "    :param train: if True the optimizer will be called to train the net on provided inputs and labels\n",
    "    :return: loss (cost) and prediction error throughout the whole data set and if save = True: also returns the list of predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_list = [] # list for prediction results\n",
    "    loss_val_sum = 0 # sum of the loss function throughout the whole data\n",
    "    error_val_sum = 0 # sum of the relative error function throughout the whole data\n",
    "    error_val_mean = 0\n",
    "    prefix = \"\"\n",
    "    \n",
    "    # number of batches == number of iterations to go through the whole dataset once\n",
    "    n_batches = (len(inputs)-par.delay-2)//(batch_size*par.pred_step)\n",
    "    remainder = (len(inputs)-par.delay-2)%(batch_size*par.pred_step)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # get batch of data\n",
    "#        if i == n_batches:\n",
    "#            # last batch\n",
    "#            x_batch, y_batch = create_batch(inputs,labels,remainder,-1/par.pred_step)\n",
    "#        else:\n",
    "        # batches before last\n",
    "        x_batch, y_batch = create_batch(inputs,labels,batch_size,i)\n",
    "\n",
    "        \n",
    "        # dropout at training\n",
    "        if par.dropout:\n",
    "            feed_dict_train = {x: x_batch, y: y_batch, input_kP: par.input_keepProb, \n",
    "                               output_kP: par.output_keepProb, recurrent_kP: par.recurrent_keepProb}\n",
    "        else:\n",
    "            feed_dict_train = {x: x_batch, y: y_batch, input_kP: 1.0, output_kP: 1.0, recurrent_kP: 1.0}\n",
    "            \n",
    "        # no dropout at testing and validation\n",
    "        feed_dict_test = {x: x_batch, y: y_batch, input_kP: 1.0, output_kP: 1.0, recurrent_kP: 1.0}\n",
    "            \n",
    "        # train the net on the data\n",
    "        if train:\n",
    "            prefix = \"Training_\" # for summary writer\n",
    "            session.run(optimizer,feed_dict=feed_dict_train) # run the optimization on the current batch\n",
    "#            loss_val, prediction_val, error_val = session.run(\n",
    "#                (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_train)\n",
    "        else:\n",
    "            prefix = \"Testing_\" # for summary writer\n",
    "#            loss_val, prediction_val, error_val = session.run(\n",
    "#                (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_test)\n",
    "            \n",
    "        loss_val, prediction_val, error_val = session.run(\n",
    "            (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_test)\n",
    "        \n",
    "        # prediction_val is a list of length \"par.pred_step\" with arrays of \"batch_size\" output values\n",
    "        # convert to numpy array of shape (batch_size, par.pred_step)\n",
    "        prediction_val = np.array(prediction_val)\n",
    "        # reshape the array to a vector of shape (1, par.pred_step*batch_size)\n",
    "        prediction_val = np.reshape(prediction_val, (1, par.pred_step*batch_size))\n",
    "        \n",
    "        loss_val_sum += loss_val # sum the losses across the batches\n",
    "        error_val_sum += error_val # sum the errors across the batches\n",
    "            \n",
    "        # save the results\n",
    "        if save:\n",
    "            # save the batch predictions to a list\n",
    "            prediction_list.extend(prediction_val[0,:])\n",
    "            \n",
    "    # the mean value of loss (throughout all the batches) in current epoch \n",
    "    loss_val_mean = loss_val_sum/n_batches        \n",
    "    # the mean of relative errors\n",
    "    error_val_mean = error_val_sum/n_batches\n",
    "        \n",
    "    # Create a new Summary object for sum of losses and mean of errors\n",
    "    loss_summary = tf.Summary()\n",
    "    error_summary = tf.Summary()\n",
    "    loss_summary.value.add(tag=\"{}Loss\".format(prefix), simple_value=loss_val_mean)\n",
    "    error_summary.value.add(tag=\"{}Error\".format(prefix), simple_value=error_val_mean)\n",
    "\n",
    "    # Add it to the Tensorboard summary writer\n",
    "    # Make sure to specify a step parameter to get nice graphs over time\n",
    "    summary_writer.add_summary(loss_summary, epoch)\n",
    "    summary_writer.add_summary(error_summary, epoch)\n",
    "\n",
    "    return loss_val_mean, error_val_mean, prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(loss_val,epoch,stop_epochs,check_every):\n",
    "    \"\"\"\n",
    "    Save the model coefficients if the data loss function value is better than the last loss function value.\n",
    "\n",
    "    :param loss_val: current value of loss function\n",
    "    :param epoch: current epoch\n",
    "    :param stop_epochs: number of epochs after which the training is stopped if the loss is not improving\n",
    "    :param check_every: define the period in epochs at which to check the loss value (and at which to save the data)\n",
    "    :return: the epoch at which the best loss was and the value of the loss (ergo at which the last checkpoint was created)\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_training = False\n",
    "    \n",
    "    # initialize function attributes\n",
    "    if not hasattr(early_stopping,\"best_loss\"):\n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.best_epoch = epoch\n",
    "    \n",
    "    # saving if the loss val is better than the last\n",
    "    if stop_epochs > 0 and epoch % check_every == 0:\n",
    "        # if loss val is better than best_loss save the model parameters\n",
    "        if loss_val < early_stopping.best_loss:\n",
    "            saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow') \n",
    "            early_stopping.best_loss = loss_val\n",
    "            early_stopping.best_epoch = epoch\n",
    "            print(\"Model saved at epoch {} \\nwith testing loss: {}.\".format(epoch,loss_val))\n",
    "        else:\n",
    "            print(\"Model NOT saved at epoch {} \\nwith testing loss: {}\".format(epoch,loss_val))\n",
    "            \n",
    "        print(\"____________________________\")\n",
    "    \n",
    "        # if the loss didn't improve for the last stop_epochs number of epochs then the training process will stop\n",
    "        if (epoch - early_stopping.best_epoch) >= stop_epochs:\n",
    "            stop_training = True \n",
    "    \n",
    "    # only saving\n",
    "    if stop_epochs <= 0 and epoch % check_every == 0:\n",
    "        saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "        early_stopping.best_loss = -1.0\n",
    "        early_stopping.best_epoch = -1\n",
    "        print(\"Model saved at epoch {}\".format(epoch))\n",
    "        print(\"____________________________\")\n",
    "        \n",
    "    \n",
    "    return early_stopping.best_loss, early_stopping.best_epoch, stop_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING THE NETWORK\n",
    "\n",
    "# initializer of TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# define unique name for log directory (one-step and multi-step are in sepparate directories)\n",
    "now = datetime.now()\n",
    "if par.pred_step > 1:\n",
    "    logdir = \"./logs/multi-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "else:\n",
    "    logdir = \"./logs/one-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "print(now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "for repeat in range(par.n_repeats):\n",
    "    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "\n",
    "        # take note of time at the start of training\n",
    "        time_train_start = time.time()\n",
    "        \n",
    "        # initialize helping variables\n",
    "        stop_training = False\n",
    "        best_epoch = par.n_epochs\n",
    "        \n",
    "        # reset instance variables\n",
    "        early_stopping.best_loss = 10000\n",
    "        early_stopping.best_epoch = 0\n",
    "\n",
    "        # Restore variables from disk if continueLearning is True.\n",
    "        if continueLearning:\n",
    "            # restoring variables will also initialize them\n",
    "            saver.restore(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "            print(\"Model restored.\")\n",
    "        else:\n",
    "            session.run(init) # initialize variables\n",
    "\n",
    "\n",
    "        # Create a SummaryWriter to output summaries and the Graph\n",
    "        # in console run 'tensorboard --logdir=./logs/'\n",
    "        summary_logdir = logdir + \"/\" + str(repeat)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=summary_logdir, graph=session.graph)\n",
    "\n",
    "        for epoch in range(par.n_epochs):\n",
    "            # TRAINING\n",
    "            loss_val, error_val, _ = run_model(x_train,y_train,par.train_batch_size,save=False,train=True)\n",
    "\n",
    "            # TESTING\n",
    "            loss_val_test, error_val_test, _ = run_model(x_test,y_test,par.test_batch_size,save=False,train=False)\n",
    "\n",
    "            # write the summaries of testing data at epoch in TensorBoard\n",
    "    #        summary_writer.add_summary(summary_test, epoch)\n",
    "\n",
    "            # increment global step for decaying learning rate at each epoch\n",
    "            session.run(increment_global_step_op)\n",
    "\n",
    "            # Printing the results at every \"par.n_epochs//10\" epochs\n",
    "            if epoch % (par.n_epochs//10) == 0:\n",
    "                print(\"Epoch: {}\".format(epoch))\n",
    "                print(\"TRAINING Loss: {}\".format(loss_val))\n",
    "                print(\"TRAINING Error: {}\".format(error_val))\n",
    "                print(\"TESTING Loss: {}\".format(loss_val_test))\n",
    "                print(\"TESTING Error: {}\".format(error_val_test))\n",
    "                # flush the summary data into TensorBoard\n",
    "                # summary_writer.flush()\n",
    "\n",
    "            # Checking the model loss_value for early stopping each \"check_every\" epochs\n",
    "            # save the trained net and variables for later use if the test loss_val is better than the last saved one\n",
    "            best_loss, best_epoch, stop_training = early_stopping(loss_val_test,epoch,par.stop_epochs,par.check_every)\n",
    "\n",
    "            # Stop the training process\n",
    "            if stop_training:\n",
    "                print(\"The training process stopped prematurely at epoch {}.\".format(epoch))\n",
    "                break\n",
    "                \n",
    "        # take note of time at the end of training\n",
    "        time_train_end = time.time()\n",
    "        \n",
    "        # print training time per epoch\n",
    "        time_train = time_train_end - time_train_start\n",
    "        print(\"The training took {} seconds.\".format(time_train))\n",
    "        print(\"Average {} seconds/epoch\".format(time_train/epoch))\n",
    "\n",
    "\n",
    "    # Restoring the model coefficients with best results\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        # restore the net coefficients with the lowest loss value\n",
    "        saver.restore(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "        print('Restored model coefficients at epoch {} with TESTING loss val: {:.4f}'.format(best_epoch, best_loss))\n",
    "\n",
    "        # run the trained net with best coefficients on all time-series and save the results\n",
    "        loss_val, error_val, prediction_list = run_model(x_train,y_train,par.train_batch_size,save=True,train=False)\n",
    "        loss_val_test, error_val_test, prediction_list_test = run_model(x_test,y_test,par.test_batch_size,save=True,train=False)\n",
    "        loss_val_validation, error_val_validation, prediction_list_validation = run_model(x_validation,y_validation,par.val_batch_size,save=True,train=False)\n",
    "    \n",
    "\n",
    "    # printing parameters and results to console\n",
    "    results = '''Timestamp: {}\n",
    "_____________________________________________\n",
    "Net parameters:\n",
    "{}\n",
    "_____________________________________________\n",
    "Results: \\n\n",
    "Best epoch ...................... {:4d}\n",
    "TRAINING Loss ................... {:11.6f}\n",
    "TRAINING Error .................. {:11.6f}\n",
    "TESTING Loss .................... {:11.6f}\n",
    "TESTING Error ................... {:11.6f}\n",
    "VALIDATION Loss ................. {:11.6f}\n",
    "VALIDATION Error ................ {:11.6f}\n",
    "_____________________________________________'''.format(now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                                                       ,par\n",
    "                                                       ,best_epoch\n",
    "                                                       ,loss_val\n",
    "                                                       ,error_val\n",
    "                                                       ,loss_val_test\n",
    "                                                       ,error_val_test\n",
    "                                                       ,loss_val_validation\n",
    "                                                       ,error_val_validation)\n",
    "\n",
    "    print(results)\n",
    "\n",
    "\n",
    "    # saving results to log file in logdir\n",
    "    file = \"{}log{}.txt\".format(logdir,repeat)\n",
    "    with open(file, mode='w') as f:\n",
    "        f.write(results)\n",
    "\n",
    "    # Shift the predictions \"par.delay\" time-steps to the right\n",
    "    prediction_train = np.array(prediction_list)\n",
    "    prediction_test = np.array(prediction_list_test)\n",
    "    prediction_validation = np.array(prediction_list_validation)\n",
    "\n",
    "    print(np.shape(prediction_train))\n",
    "\n",
    "    prediction_train = np.pad(prediction_train,pad_width=((par.delay,0))\n",
    "                              ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "    prediction_test = np.pad(prediction_test,pad_width=((par.delay,0))\n",
    "                             ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "    prediction_validation = np.pad(prediction_validation,pad_width=((par.delay,0))\n",
    "                             ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "\n",
    "    print(np.shape(prediction_train))\n",
    "\n",
    "\n",
    "    def denormalize(labels):\n",
    "        \"\"\"\n",
    "        Denormalize target values from interval (-1,1) to original values\n",
    "\n",
    "        :param labels: values of labels to be denormalized\n",
    "        :return: denormalized values of labels\n",
    "        \"\"\"\n",
    "\n",
    "        # denormalized_labels = min_data + labels*(max_data - min_data)\n",
    "        # denormalize the labels from (-1,1)\n",
    "        denormalized_labels = mean_data + labels*std_data\n",
    "\n",
    "        return denormalized_labels\n",
    "\n",
    "    y_train_denorm = denormalize(y_train)\n",
    "    y_test_denorm = denormalize(y_test)\n",
    "    y_validation_denorm = denormalize(y_validation)\n",
    "\n",
    "\n",
    "    # Plot the results\n",
    "    def plot_results(predictions, targets, selected_data):\n",
    "        \"\"\"\n",
    "        plot the predictions and target values to one figure\n",
    "\n",
    "        :param predictions: the predicted values from the net\n",
    "        :param targets: the actual values of the time series\n",
    "        :param selected_data: selector of input data (1 == Elbe flow, 2 == Saugeen flow) \n",
    "        :return: plot of predictions and target values\n",
    "        \"\"\"\n",
    "        plt.plot(predictions)\n",
    "        plt.plot(targets, alpha=0.6)\n",
    "        plt.legend(['predictions', 'targets'])\n",
    "        if selected_data == 1:\n",
    "            plt.xlabel('time (weeks)')\n",
    "            plt.ylabel('flow rate (normalized)')\n",
    "        elif selected_data == 2:\n",
    "            plt.xlabel('time (days)')\n",
    "            plt.ylabel('flow rate (unknown)')        \n",
    "        plt.draw()\n",
    "\n",
    "    f_training = plt.figure()\n",
    "    plot_results(prediction_train[plot_start:plot_end], y_train_denorm[plot_start:plot_end], selected_data)\n",
    "    plt.title('TRAINING')\n",
    "\n",
    "    f_testing = plt.figure()\n",
    "    plot_results(prediction_test[plot_start:plot_end], y_test_denorm[plot_start:plot_end], selected_data)\n",
    "    plt.title('TESTING')\n",
    "\n",
    "    f_validation = plt.figure()\n",
    "    plot_results(prediction_validation[plot_start:plot_end], y_validation_denorm[plot_start:plot_end], selected_data)\n",
    "    plt.title('VALIDATION')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the figures:\n",
    "    img_save_dir = \"{}IMG\".format(logdir)\n",
    "    save_dir_path = os.path.join(os.curdir, img_save_dir)\n",
    "    os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "    f_training.savefig(save_dir_path + \"/training{}.pdf\".format(repeat), bbox_inches='tight')\n",
    "    f_testing.savefig(save_dir_path + \"/testing{}.pdf\".format(repeat), bbox_inches='tight')\n",
    "    f_validation.savefig(save_dir_path + \"/validation{}.pdf\".format(repeat), bbox_inches='tight')\n",
    "\n",
    "    print(\"Figures saved to: {}\".format(save_dir_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following cells are for debugging purposes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "par.pred_step = 4\n",
    "par.delay = 4\n",
    "\n",
    "x_batch, y_batch = create_batch(x_train,y_train,4,1)\n",
    "\n",
    "print(np.shape(x_batch),np.shape(y_batch))\n",
    "print(x_batch,'\\n----\\n', y_batch)\n",
    "\n",
    "x_batch, y_batch = create_batch(x_train,y_train,4,2)\n",
    "\n",
    "print(np.shape(x_batch),np.shape(y_batch))\n",
    "print(x_batch,'\\n-----\\n', y_batch)\n",
    "\n",
    "\n",
    "# x_train[7*(par.pred_step+par.delay-1):8*(par.pred_step+par.delay-1)]\n",
    "\n",
    "print(x_train[100:200:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    " - Variable length last batch to make use of all the available data!!!\n",
    " - plot every par.n_epochs//10 into tensorboard (different charts?)\n",
    " - try chaning par.hidden_size --- DONE\n",
    " - try chaning par.delay\n",
    " - try chaning par.n_lstm_layers\n",
    " - regularization (not necessarry)\n",
    "\n",
    "\n",
    " - test and validation --- DONE\n",
    " - drift??? --- DONE\n",
    " - Show the graph in TensorBoard -- DONE\n",
    " - add Dropout layer -- DONE\n",
    " - divide loss sum by n_iter to make it a loss val mean !!! --- DONE\n",
    " - add relative error between the predictions and targets --- DONE\n",
    "\n",
    "#### Hyperparameter tuning\n",
    " - smaller batch size is better because you have to predict less things before updating weights\n",
    "   but makes the training values more unstable\n",
    " - more prediction steps are worse\n",
    " - bigger delay?\n",
    " - number of stacked layers? - two is enough\n",
    " - smaller hidden size => faster, easier learning, less overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decayed_learning_rate_test = 0.01 * 0.90 ** (2000 / 1000)\n",
    "print(decayed_learning_rate_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "par.pred_step = 1\n",
    "# print(create_batch(x_train,y_train, 21,-1//par.pred_step))\n",
    "\n",
    "print(x_train[-3:-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# list of np arrays\n",
    "vectors = [np.array([1, 2, 3])]*5\n",
    "\n",
    "print(vectors)\n",
    "\n",
    "np_vectors = np.array(vectors)\n",
    "\n",
    "print(np.shape(np_vectors))\n",
    "\n",
    "np_vectors = np.reshape(np_vectors,(1,3*5))\n",
    "print(np_vectors)\n",
    "print(np.shape(np_vectors))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(x_train)\n",
    "#print(n_iter)\n",
    "print(14)\n",
    "print((len(x_train)-par.delay)//(par.batch_size*par.pred_step))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.mean(x_train)\n",
    "print(np.shape(x_test), np.shape(x_test[:4000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print only tensor v1 in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file('./checkpoints/Multi-Step_LSTMforPredictingLabeFlow',\n",
    "                                      tensor_name='output_layer/weights_out', \n",
    "                                      all_tensors=False, \n",
    "                                      all_tensor_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_val = [1,2,3,4]\n",
    "\n",
    "pred_list = list(list_val[-i-1] for i in range(par.pred_step))\n",
    "pred_list = pred_list[::-1]\n",
    "\n",
    "print(pred_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Early stopping\n",
    "def early_stopping(loss_val,epoch):\n",
    "    \"\"\"\n",
    "    Save the model coefficients if the data loss function value is better than the last\n",
    "    loss function value.\n",
    "    Return the epoch at which the best loss was and the value of the loss (ergo at which the last checkpoint was created)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize function attributes\n",
    "    if not hasattr(early_stopping,\"best_loss\"):\n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.epoch = 0\n",
    "    \n",
    "    # compare the loss_val\n",
    "    if loss_val < early_stopping.best_loss:\n",
    " #       saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow') \n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.epoch = epoch\n",
    "        \n",
    "    return early_stopping.best_loss, early_stopping.epoch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(early_stopping(0.5,0))\n",
    "print(early_stopping(0.6,10))\n",
    "print(early_stopping(0.4,20))\n",
    "print(early_stopping(0.8,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define net parameters\n",
    "class params:\n",
    "    # initialization of instance variables\n",
    "    def __init__(self,n_lstm_layers,hidden_size,delay,pred_step,batch_size,n_epochs,stop_epochs,init_lr):\n",
    "        self.input_size = 1 # number of input features (we only follow one variable == flow)\n",
    "        self.num_classes = 1 # number of output classes (we wan't specific value, not classes, so this is 1)\n",
    "        self.target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "        self.n_lstm_layers = n_lstm_layers # number of vertically stacked LSTM layers\n",
    "        self.hidden_size = hidden_size # hidden state vector size in LSTM cell\n",
    "        self.delay = delay # the number of time-steps from which we are going to predict the next step\n",
    "        self.pred_step = pred_step # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "        self.batch_size = batch_size # number of inputs in one batch\n",
    "        self.n_epochs = n_epochs # number of epochs\n",
    "        self.stop_epochs = stop_epochs # if the loss value doesn't improve over the last stop_epochs, the training process will stop\n",
    "        self.init_lr = init_lr # initial learning rate for Adam optimizer (training algorithm)\n",
    "        self.net_unroll_size = delay + pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "        \n",
    "    # how will the class object be represented in string form (eg. when called with print())\n",
    "    def __str__(self):\n",
    "        answer = '''\n",
    "Input size ...................... {:4d}\n",
    "Number of classes ............... {:4d}\n",
    "Target shift .................... {:4d}\n",
    "Number of stacked LSTM layers ... {:4d}\n",
    "Hidden state size ............... {:4d}\n",
    "Delay ........................... {:4d}\n",
    "Number of prediciton steps....... {:4d}\n",
    "Batch size ...................... {:4d}\n",
    "Maximum number of epochs ........ {:4d}\n",
    "Early stopping epochs ........... {:4d}\n",
    "Initial learning rate ........... {:9.4f}'''.format(self.input_size\n",
    "                                              ,self.num_classes\n",
    "                                              ,self.target_shift\n",
    "                                              ,self.n_lstm_layers\n",
    "                                              ,self.hidden_size\n",
    "                                              ,self.delay\n",
    "                                              ,self.pred_step\n",
    "                                              ,self.batch_size\n",
    "                                              ,self.n_epochs\n",
    "                                              ,self.stop_epochs\n",
    "                                              ,self.init_lr)\n",
    "        return answer\n",
    "        # return str(vars(self))\n",
    "\n",
    "# net and training parameter specification\n",
    "par = params(2,20,10,1,5,100,20,0.001)\n",
    "\n",
    "print(par)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p1 = params()\n",
    "p2 = params()\n",
    "p1.par.batch_size = 10\n",
    "p2.par.batch_size = 20\n",
    "\n",
    "print(p1.par.batch_size, p2.par.batch_size)\n",
    "\n",
    "p1.par.delay = 5\n",
    "\n",
    "print(p1.par.delay)\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variables = vars(p1)\n",
    "#print(variables)\n",
    "print(\"Params: {}\".format(p1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p1.par.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10)\n",
    "\n",
    "print(x)\n",
    "\n",
    "mov_avg = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "    mov_avg = (mov_avg + x[i])/(i+1)\n",
    "    \n",
    "error = np.sum(x)/len(x)\n",
    "\n",
    "print(mov_avg, error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_size = 208\n",
    "par.batch_size = 4\n",
    "par.delay = 51\n",
    "par.pred_step = 3\n",
    "\n",
    "n_batches = (len(x_train)-par.delay-2)//(par.batch_size*par.pred_step)\n",
    "\n",
    "print(create_batch(x_train, y_train, par.batch_size,n_batches))\n",
    "\n",
    "print(y_train[-2])\n",
    "\n",
    "index = 0\n",
    "i = 0\n",
    "step = index*(batch_size*par.pred_step)\n",
    "\n",
    "y_batch = np.reshape(y_train[step+par.delay+i*par.pred_step+1:step+par.delay+i*par.pred_step+1+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "\n",
    "print(y_batch, y_train[40:51])\n",
    "\n",
    "\n",
    "print(n_batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
