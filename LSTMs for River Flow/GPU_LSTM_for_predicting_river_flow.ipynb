{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of river flow using RNN with LSTM architecture"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install scipy openpyxl matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "# from tensorflow.nn import dynamic_rnn\n",
    "from tensorflow.python.tools import inspect_checkpoint as chkp # import the inspect_checkpoint library\n",
    "# import tensorboard as tb\n",
    "import numpy as np\n",
    "import scipy.io as sio # for working with .mat files\n",
    "# import pandas as pd # for working with .xlsx files\n",
    "from openpyxl import load_workbook # for working with .xlsx files\n",
    "import matplotlib.pyplot as plt # for plotting the data\n",
    "from datetime import datetime # for keeping separate TB logs for each run\n",
    "import os, sys\n",
    "import textwrap\n",
    "import time # for measuring time cost\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# NAMED TUPLE CLASS ALTERNATIVE FOR DEFINING PARAMS\n",
    "fields = ['input_size',\n",
    "          'num_classes',\n",
    "          'target_shift',\n",
    "          'n_lstm_layers',\n",
    "          'hidden_size',\n",
    "          'delay',\n",
    "          'pred_step',\n",
    "          'batch_size', # tuple of 3 values (train,test,validation)\n",
    "          'n_epochs',\n",
    "          'stop_epochs',\n",
    "          'check_every',\n",
    "          'init_lr',\n",
    "          'n_repeats',\n",
    "          'net_unroll_size',\n",
    "          'dropout',\n",
    "          'keep_prob']  # tuple of 3 values (input,output,recurrent)\n",
    "params = namedtuple('params',fields)\n",
    "\n",
    "par = params(input_size=1, num_classes=1, target_shift=1,\n",
    "             n_lstm_layers=2, hidden_size=10, delay=10,\n",
    "             pred_step=1, batch_size=(5,5,5), n_epochs=100,\n",
    "             stop_epochs=20, check_every=10, init_lr=0.001,\n",
    "             n_repeats=1, net_unroll_size=10, dropout=False,\n",
    "             keep_prob=(1.0,1.0,1.0))\n",
    "\n",
    "# CREATING ORDERED DICTIONARY FROM THE TUPLE AND PRINTING IT'S ITMES WITH KEYS\n",
    "for key, value in par._asdict().items():\n",
    "    print(key + ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define net parameters\n",
    "class params:\n",
    "    # initialization of instance variables\n",
    "    def __init__(self,n_lstm_layers=2,hidden_size=10,delay=10,pred_step=1,train_batch_size=5,test_batch_size=5,val_batch_size=5,n_epochs=100,stop_epochs=20,check_every=10,init_lr=0.001,n_repeats=1,dropout=False,input_keepProb=1,output_keepProb=1,recurrent_keepProb=1):\n",
    "        self.input_size = 1 # number of input features (we only follow one variable == flow)\n",
    "        self.num_classes = 1 # number of output classes (we wan't specific value, not classes, so this is 1)\n",
    "        self.target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "        self.n_lstm_layers = n_lstm_layers # number of vertically stacked LSTM layers\n",
    "        self.hidden_size = hidden_size # hidden state vector size in LSTM cell\n",
    "        self.delay = delay # the number of time-steps from which we are going to predict the next step\n",
    "        self.pred_step = pred_step # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "        self.train_batch_size = train_batch_size # number of inputs in one training batch\n",
    "        self.test_batch_size = test_batch_size # number of inputs in one testing batch\n",
    "        self.val_batch_size = val_batch_size # number of inputs in one validation batch\n",
    "        self.n_epochs = n_epochs # number of epochs\n",
    "        self.stop_epochs = stop_epochs # if the loss value doesn't improve over the last stop_epochs, the training process will stop\n",
    "        self.check_every = check_every # how often to check for loss value in early stopping\n",
    "        self.init_lr = init_lr # initial learning rate for Adam optimizer (training algorithm)\n",
    "        self.n_repeats = n_repeats # number of repeats of the whole training and validation process with the same params\n",
    "        self.net_unroll_size = self.delay + self.pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "        # FIGHTING OVERFITTING:\n",
    "        self.dropout = dropout # if true the dropout is applied on inputs, outputs and recurrent states of cells\n",
    "        self.input_keepProb = input_keepProb # (dropout) probability of keeping the input\n",
    "        self.output_keepProb = output_keepProb # (dropout) probability of keeping the output\n",
    "        self.recurrent_keepProb = recurrent_keepProb # (dropout) probability of keeping the recurrent state\n",
    "    \n",
    "    # representation of object for interpreter and debugging purposes\n",
    "    def __repr__(self):\n",
    "        return '''params(n_lstm_layers={:d},hidden_size={:d},delay={:d},pred_step={:d},\\\n",
    "train_batch_size={:d},test_batch_size={:d},val_batch_size={:d},n_epochs={:d},stop_epochs={:d},check_every={:d},init_lr={:f},dropout={},\\\n",
    "n_repeats={:d},input_keepProb={:f},output_keepProb={:f},recurrent_keepProb={:f})'''.format(self.n_lstm_layers,\n",
    "                                                                            self.hidden_size,\n",
    "                                                                            self.delay,\n",
    "                                                                            self.pred_step,\n",
    "                                                                            self.train_batch_size,\n",
    "                                                                            self.test_batch_size,\n",
    "                                                                            self.val_batch_size,\n",
    "                                                                            self.n_epochs,\n",
    "                                                                            self.stop_epochs,\n",
    "                                                                            self.check_every,\n",
    "                                                                            self.init_lr,\n",
    "                                                                            self.n_repeats,\n",
    "                                                                            self.dropout,\n",
    "                                                                            self.input_keepProb,\n",
    "                                                                            self.output_keepProb,\n",
    "                                                                            self.recurrent_keepProb)\n",
    "    \n",
    "    # how will the class object be represented in string form (eg. when called with print())\n",
    "    def __str__(self):\n",
    "        answer = '''\n",
    "Input size ...................... {:4d}\n",
    "Number of classes ............... {:4d}\n",
    "Target shift .................... {:4d}\n",
    "Number of stacked LSTM layers ... {:4d}\n",
    "Hidden state size ............... {:4d}\n",
    "Delay ........................... {:4d}\n",
    "Number of prediciton steps....... {:4d}\n",
    "Training batch size ............. {:4d}\n",
    "Testing batch size .............. {:4d}\n",
    "Validation batch size ........... {:4d}\n",
    "Maximum number of epochs ........ {:4d}\n",
    "Early stopping epochs ........... {:4d}\n",
    "Check (save) every (epochs)...... {:4d}\n",
    "Initial learning rate ........... {:9.4f}\n",
    "Dropout ......................... {}'''.format(self.input_size\n",
    "                                              ,self.num_classes\n",
    "                                              ,self.target_shift\n",
    "                                              ,self.n_lstm_layers\n",
    "                                              ,self.hidden_size\n",
    "                                              ,self.delay\n",
    "                                              ,self.pred_step\n",
    "                                              ,self.train_batch_size\n",
    "                                              ,self.test_batch_size\n",
    "                                              ,self.val_batch_size\n",
    "                                              ,self.n_epochs\n",
    "                                              ,self.stop_epochs\n",
    "                                              ,self.check_every\n",
    "                                              ,self.init_lr\n",
    "                                              ,self.dropout)\n",
    "        \n",
    "        dropout_answer = '''\n",
    "Input keep probability .......... {:7.2f}\n",
    "Output keep probability ......... {:7.2f}\n",
    "Recurrent keep probability ...... {:7.2f}'''.format(self.input_keepProb\n",
    "                                                 ,self.output_keepProb\n",
    "                                                 ,self.recurrent_keepProb)\n",
    "        \n",
    "        if self.dropout:\n",
    "            return answer + dropout_answer\n",
    "        else:\n",
    "            return answer\n",
    "\n",
    "# net and training parameter specification\n",
    "par = params(n_lstm_layers = 2\n",
    "            ,hidden_size = 16\n",
    "            ,delay = 32\n",
    "            ,pred_step = 1\n",
    "            ,train_batch_size=16 # (https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/)\n",
    "            ,test_batch_size=4\n",
    "            ,val_batch_size=4\n",
    "            ,n_epochs=1000\n",
    "            ,stop_epochs=50 # if <= 0 then early stopping is disabled and check every sets the saving period\n",
    "            ,check_every=10\n",
    "            ,init_lr=0.001\n",
    "            ,n_repeats=1\n",
    "            ,dropout=False\n",
    "            ,input_keepProb=1.0\n",
    "            ,output_keepProb=1.0\n",
    "            ,recurrent_keepProb=1.0)\n",
    "    \n",
    "\n",
    "# enable/disable L2 regularization\n",
    "regularization = False # NOT IMPLEMENTED YET (go to: https://stackoverflow.com/questions/37869744/tensorflow-lstm-regularization)\n",
    "\n",
    "# continue training the model with saved variables from previeous training\n",
    "continueLearning = False\n",
    "\n",
    "# decaying learning rate constants (for exponential decay)\n",
    "# Adam already has adaptive learning rate for individual weights \n",
    "# but it can be combined with decaying learning rate for faster convergence\n",
    "decay_steps = par.n_epochs//10 # every \"n_epochs//10\" epochs the learning rate is reduced\n",
    "decay_rate = 1 # the base of the exponential (rate of the decay ... 1 == no decay)\n",
    "\n",
    "# deciding which device (CPU/GPU) should be used for tf operations\n",
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    device = \"/gpu:0\"\n",
    "else:\n",
    "    device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input size ......................    1\n",
      "Number of classes ...............    1\n",
      "Target shift ....................    1\n",
      "Number of stacked LSTM layers ...    2\n",
      "Hidden state size ...............   16\n",
      "Delay ...........................   32\n",
      "Number of prediciton steps.......    1\n",
      "Training batch size .............   16\n",
      "Testing batch size ..............    4\n",
      "Validation batch size ...........    4\n",
      "Maximum number of epochs ........ 1000\n",
      "Early stopping epochs ...........   50\n",
      "Check (save) every (epochs)......   10\n",
      "Initial learning rate ...........    0.0010\n",
      "Dropout ......................... False\n"
     ]
    }
   ],
   "source": [
    "print(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select input data for the network\n",
    "# 1) ... Weekly Elbe flow (normalized)\n",
    "# 2) ... Daily Saugeen flow (unknown)\n",
    "selected_data = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) DATA from prof. A. Procházka:\n",
    "* **url:**: http://uprt.vscht.cz/prochazka/pedag/Data/dataNN.zip\n",
    "* **name**: Weekly Elbe river flow\n",
    "* **Provider source:** Prof. Ing. Aleš Procházka, CSc\n",
    "* **Span:** 313 weeks ~ 6 years of data\n",
    "* **Data size:** 313 values\n",
    "* **Already normalized** to 0 mean and 1 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if selected_data == 1:\n",
    "    # load data from Q.mat\n",
    "    filename = './datasets/Q.MAT'\n",
    "    data = sio.loadmat(filename) # samples were gathered with period of one week\n",
    "\n",
    "    # convert to np array\n",
    "    data = np.array(data['Q'],dtype=np.float32)\n",
    "\n",
    "    print(np.shape(data))\n",
    "\n",
    "    # normalize the data to interval (0,1)\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "    # shift the data to start at 0.001 (for relative error counting purposes)\n",
    "#    data = np.subtract(data,min_data)+0.001\n",
    "    # data = np.divide(np.subtract(data,min_data),np.subtract(max_data,min_data)).flatten()\n",
    "    # normalize the data to interval (-1,1) (cca 0 mean and 1 variance)\n",
    "    mean_data = np.mean(data) # mean\n",
    "    std_data = np.std(data) # standard deviation\n",
    "    data = np.divide(np.subtract(data,mean_data),std_data).flatten()\n",
    "\n",
    "    # divide the data into training, testing and validation part\n",
    "    weeks_in_year = 52.1775\n",
    "    years_in_data = 313/weeks_in_year\n",
    "\n",
    "    years_in_train = int(years_in_data*0.7) # 70% of data rounded to the number of years\n",
    "    years_in_test = int(np.ceil(years_in_data*0.15)) # 15% of data rounded to the number of years\n",
    "\n",
    "    weeks_train = int(years_in_train*weeks_in_year) # number of weeks in training data\n",
    "    weeks_test = int(years_in_test*weeks_in_year) # number of weeks in testing data\n",
    "\n",
    "    end_of_train = weeks_train\n",
    "    end_of_test = weeks_train + weeks_test\n",
    "\n",
    "    x_train = data[:end_of_train]\n",
    "    x_test = data[end_of_train:end_of_test]\n",
    "    x_validation = data[end_of_test:]\n",
    "\n",
    "    plot_start = 0\n",
    "    plot_end = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) DATA from Time Series Data Library:\n",
    "* **url:** https://datamarket.com/data/set/235a/mean-daily-saugeen-river-flows-jan-01-1915-to-dec-31-1979#!ds=235a&display=line\n",
    "* **name:** Mean daily Saugeen River (Canada) flows\n",
    "* **Provider source:** Hipel and McLeod (1994)\n",
    "* **Span:** Jan 01, 1915 to Dec 31, 1979\n",
    "* **Data size:** 23741 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mean daily saugeen River flows,']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: DeprecationWarning: Call to deprecated function get_sheet_names (Use wb.sheetnames).\n",
      "C:\\Users\\Martin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:10: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n"
     ]
    }
   ],
   "source": [
    "if selected_data == 2:\n",
    "    # load excel spreadsheet with openpyxl:\n",
    "    filename = './datasets/sugeen-river-flows.xlsx'\n",
    "    xl = load_workbook(filename)\n",
    "\n",
    "    # print sheet names:\n",
    "    print(xl.get_sheet_names())\n",
    "\n",
    "    # get sheet:\n",
    "    sheet = xl.get_sheet_by_name('Mean daily saugeen River flows,')\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # fill a list with values from cells:\n",
    "    for cell in sheet['B16:B23756']:\n",
    "        data.append(cell[0].value)\n",
    "\n",
    "    # convert list to numpy array and reshape to a column vector\n",
    "    data = np.array(data)\n",
    "    data = np.reshape(data,(1,-1))\n",
    "\n",
    "    # normalize the data to interval (0,1) <- DONT!\n",
    "    min_data = np.min(data)\n",
    "    max_data = np.max(data)\n",
    "    # data = np.divide(np.subtract(data,min_data),np.subtract(max_data,min_data)).flatten()\n",
    "    # !!! CENTERING data:\n",
    "    # normalize the data to interval (-1,1) (cca 0 mean and 1 variance)\n",
    "    # data = data[0,:120]\n",
    "    mean_data = np.mean(data) # mean\n",
    "    std_data = np.std(data) # standard deviation\n",
    "    data = np.divide(np.subtract(data,mean_data),std_data).flatten()\n",
    "\n",
    "    # divide the data into training, testing and validation part\n",
    "    days_in_data = np.shape(data)[0]\n",
    "    days_in_year = 365.25\n",
    "    years_in_data = days_in_data/days_in_year\n",
    "\n",
    "    years_in_train = int(years_in_data*0.7) # 70% of data rounded to the number of years\n",
    "    years_in_test = int(np.ceil(years_in_data*0.15)) # 15% of data rounded to the number of years\n",
    "\n",
    "    days_train = int(years_in_train*days_in_year) # number of days in training data\n",
    "    days_test = int(years_in_test*days_in_year) # number of days in testing data\n",
    "\n",
    "    end_of_train = days_train\n",
    "    end_of_test = days_train + days_test\n",
    "\n",
    "    x_train = data[:end_of_train]\n",
    "    x_test = data[end_of_train:end_of_test]\n",
    "    x_validation = data[end_of_test:]\n",
    "\n",
    "    plot_start = int(days_in_year*3)\n",
    "    plot_end = int(days_in_year*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the shifted time-series (targets)\n",
    "y_train = np.roll(x_train, par.target_shift)\n",
    "y_test = np.roll(x_test, par.target_shift)\n",
    "y_validation = np.roll(x_validation, par.target_shift)\n",
    "\n",
    "# delete the first elements of the time series that were reintroduced from the end of the timeseries\n",
    "y_train[:par.target_shift] = 0\n",
    "y_test[:par.target_shift] = 0\n",
    "y_validation[:par.target_shift] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset TensorFlow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(device):\n",
    "    # define tensorflow constants\n",
    "    min_of_data = tf.constant(min_data, dtype=tf.float32, name='min_of_data')\n",
    "    max_of_data = tf.constant(max_data, dtype=tf.float32, name='max_of_data')\n",
    "    mean_of_data = tf.constant(mean_data, dtype=tf.float32, name='mean_of_data')\n",
    "    std_of_data = tf.constant(std_data, dtype=tf.float32, name='std_of_data')\n",
    "\n",
    "    # define output weights and biases\n",
    "    with tf.name_scope(\"output_layer\"):\n",
    "        weights_out = tf.Variable(tf.random_normal([par.hidden_size,par.num_classes]),name='weights_out')\n",
    "        bias_out = tf.Variable(tf.random_normal([par.num_classes]),name='biases_out')\n",
    "\n",
    "    # define placeholders for the batches of time-series\n",
    "    x = tf.placeholder(tf.float32,[None, par.net_unroll_size, par.input_size],name='x') # batch of inputs\n",
    "    y = tf.placeholder(tf.float32,[None, par.num_classes, par.pred_step],name='y') # batch of labels\n",
    "\n",
    "    # define placeholders for dropout keep probabilities\n",
    "    input_kP = tf.placeholder(tf.float32,name='input_kP')\n",
    "    output_kP = tf.placeholder(tf.float32,name='output_kP')\n",
    "    recurrent_kP = tf.placeholder(tf.float32,name='recurrent_kP')\n",
    "    \n",
    "    # processing the input tensor from [par.batch_size,par.delay,par.input_size] to \"par.delay\" number of [par.batch_size,par.input_size] tensors\n",
    "    input=tf.unstack(x, par.net_unroll_size, 1, name='LSTM_input_list') # create list of values by unstacking one dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create an LSTM cell:\n",
    "def make_cell(hidden_size):\n",
    "    return rnn.LSTMCell(hidden_size, state_is_tuple=True, activation=tf.tanh)\n",
    "\n",
    "# define an LSTM network with 'par.n_lstm_layers' layers\n",
    "with tf.device(device):\n",
    "    with tf.name_scope(\"LSTM_layer\"):\n",
    "        lstm_cells = rnn.MultiRNNCell([make_cell(par.hidden_size) for _ in range(par.n_lstm_layers)], state_is_tuple=True)\n",
    "\n",
    "        # add dropout to the inputs and outputs of the LSTM cell (reduces overfitting)\n",
    "        lstm_cells = rnn.DropoutWrapper(lstm_cells, input_keep_prob=input_kP, output_keep_prob=output_kP, state_keep_prob=recurrent_kP)\n",
    "\n",
    "        # create static RNN from lstm_cell\n",
    "        outputs,_ = rnn.static_rnn(lstm_cells, input, dtype=tf.float32)\n",
    "    #    outputs,_ = tf.nn.dynamic_rnn(lstm_cells, x, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a list of predictions based on the last \"par.pred_step\" time-step outputs (multi-step prediction)\n",
    "with tf.device(device):\n",
    "    prediction = [tf.matmul(outputs[-i-1],weights_out) + bias_out for i in (range(par.pred_step))] # future prediction first\n",
    "    prediction = prediction[::-1] #reverse the list (one-step prediction first)\n",
    "\n",
    "    prediction = tf.reshape(prediction,[-1,par.num_classes,par.pred_step])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "# define loss function with regularization\n",
    "# loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=prediction, labels=y))\n",
    "with tf.device(device):\n",
    "    with tf.name_scope(\"loss\"):\n",
    "    #    regularization_cost = tf.reduce_sum([ tf.nn.l2_loss(v) for v in tv ])\n",
    "        loss = tf.sqrt(tf.losses.mean_squared_error(predictions=prediction,labels=y)) # RMSE (root mean squared error)\n",
    "\n",
    "# exponential decay of learning rate with epochs\n",
    "global_step = tf.Variable(1, trainable=False, name='global_step') # variable that keeps track of the step at which we are in the training\n",
    "increment_global_step_op = tf.assign(global_step, global_step+1,name='increment_global_step') # operation that increments global step by one\n",
    "\n",
    "with tf.device(device):\n",
    "    # decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "    learning_rate = tf.train.exponential_decay(par.init_lr, global_step,\n",
    "                                               decay_steps, decay_rate, staircase=True) # decay at discrete intervals\n",
    "\n",
    "    # define Adam optimizer for training of the network\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Martin\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\tf_inspect.py:55: DeprecationWarning: inspect.getargspec() is deprecated, use inspect.signature() or inspect.getfullargspec()\n",
      "  if d.decorator_argspec is not None), _inspect.getargspec(target))\n"
     ]
    }
   ],
   "source": [
    "with tf.device(device):\n",
    "    # denormalize data (from (0,1)):\n",
    "    # denormalized_prediction = min_of_data + tf.multiply(prediction, tf.subtract(max_of_data,min_of_data))\n",
    "    # denormalized_y = min_of_data + tf.multiply(y, tf.subtract(max_of_data,min_of_data))\n",
    "\n",
    "    # denormalize data (from (-1,1)): <- better\n",
    "    denormalized_prediction = mean_of_data + tf.multiply(prediction, std_of_data)\n",
    "    denormalized_y = mean_of_data + tf.multiply(y, std_of_data)\n",
    "\n",
    "    # calculate relative error of denormalized data:\n",
    "    with tf.name_scope(\"SMAPE\"):\n",
    "        SMAPE = 2*tf.reduce_mean(tf.divide(tf.abs(tf.subtract(denormalized_prediction,denormalized_y))\n",
    "                                           ,tf.add(tf.abs(denormalized_y),tf.abs(denormalized_prediction))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard summaries (visualization logs)\n",
    "# histogram summary for output weights\n",
    "w_out_summ = tf.summary.histogram(\"w_out_summary\", weights_out)\n",
    "# scalar summary for loss function\n",
    "# training_loss_summ = tf.summary.scalar(\"training_loss\",loss)\n",
    "# testing_loss_summ = tf.summary.scalar(\"testing_loss\",loss)\n",
    "# scalar summary for relative error\n",
    "# training_error_summ = tf.summary.scalar(\"relative_training_error\",SMAPE)\n",
    "# testing_error_summ = tf.summary.scalar(\"relative_testing_error\",SMAPE)\n",
    "\n",
    "# NOT USEFUL HERE\n",
    "# merge the summaries of all tf.summary calls (for TensorBoard visualization)\n",
    "# merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_batch(x_data,y_data,batch_size,index):\n",
    "    \"\"\"\n",
    "    function for generating the time-series batches\n",
    "\n",
    "    :param x_data: input data series\n",
    "    :param y_data: output data series\n",
    "    :param batch_size: size of one batch of data to feed into network\n",
    "    :param index: index of the current batch of data\n",
    "    :return: input and output batches of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_batch = np.zeros([batch_size,par.delay,par.input_size])\n",
    "    x_pad = np.zeros([batch_size,par.pred_step-1,par.input_size])\n",
    "    y_batch = np.zeros([batch_size,par.num_classes,par.pred_step])\n",
    "    \n",
    "    step = index*(batch_size*par.pred_step)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_batch[i,:,:] = np.reshape(x_data[step+i*par.pred_step:step+i*par.pred_step+par.delay],(par.delay,par.num_classes))\n",
    "        y_batch[i,:] = np.reshape(y_data[step+par.delay+i*par.pred_step+1:step+par.delay+i*par.pred_step+1+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "    \n",
    "    # the last \"par.pred_step - 1\" columns in x_batch are padded with 0\n",
    "    # because there are no inputs into the net at these time steps\n",
    "    x_batch = np.hstack((x_batch, x_pad))\n",
    "    \n",
    "#    print(x_batch)\n",
    "#    print('________________')\n",
    "#    print(y_batch)\n",
    "#    print('================')\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch(x_data,y_data,batch_size,index):\n",
    "    \"\"\"\n",
    "    function for generating the time-series batches\n",
    "\n",
    "    :param x_data: input data series\n",
    "    :param y_data: output data series\n",
    "    :param batch_size: size of one batch of data to feed into network\n",
    "    :param index: index of the current batch of data\n",
    "    :return: input and output batches of data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_batch = np.zeros([batch_size,par.delay,par.input_size])\n",
    "    x_pad = np.zeros([batch_size,par.pred_step-1,par.input_size])\n",
    "    y_batch = np.zeros([batch_size,par.num_classes,par.pred_step])\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        x_batch[i,:,:] = np.reshape(x_data[index+i:index+i+par.delay],(par.delay,par.num_classes))\n",
    "        y_batch[i,:] = np.reshape(y_data[index+i+par.delay+1:index+i+par.delay+1+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "#        x_batch[i,:,:] = np.reshape(x_data[index+i:index+i+par.delay],(par.delay,par.num_classes))\n",
    "#        y_batch[i,:] = np.reshape(y_data[index+i+par.delay:index+i+par.delay+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "    \n",
    "    # the last \"par.pred_step - 1\" columns in x_batch are padded with 0\n",
    "    # because there are no inputs into the net at these time steps\n",
    "    x_batch = np.hstack((x_batch, x_pad))\n",
    "    \n",
    "#    print(x_batch)\n",
    "#    print('________________')\n",
    "#    print(y_batch)\n",
    "#    print('================')\n",
    "    \n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[1.93896696]]]), array([[[1.55339868]]]))\n"
     ]
    }
   ],
   "source": [
    "par.delay = 1\n",
    "par.pred_step = 1\n",
    "batch_size = 1\n",
    "index = 101\n",
    "test = create_batch(x_data=x_train,y_data=y_train,batch_size=batch_size,index=index)\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(inputs,labels,batch_size,save=False,train=False):\n",
    "    \"\"\"\n",
    "    feeding the model with batches of inputs, running the optimizer for training and getting training and testing results\n",
    "\n",
    "    :param inputs: input data series\n",
    "    :param labels: output data series\n",
    "    :param batch_size: the size of the data batch\n",
    "    :param save: if True the predicted time series is returned as a list\n",
    "    :param train: if True the optimizer will be called to train the net on provided inputs and labels\n",
    "    :return: loss (cost) and prediction error throughout the whole data set and if save = True: also returns the list of predicted values\n",
    "    \"\"\"\n",
    "    \n",
    "    prediction_list = [] # list for prediction results\n",
    "    loss_val_sum = 0 # sum of the loss function throughout the whole data\n",
    "    error_val_sum = 0 # sum of the relative error function throughout the whole data\n",
    "    error_val_mean = 0\n",
    "    prefix = \"\"\n",
    "    \n",
    "    # number of batches == number of iterations to go through the whole dataset once\n",
    "    n_batches = (len(inputs)-par.delay-2)//(batch_size*par.pred_step)\n",
    "    remainder = (len(inputs)-par.delay-2)%(batch_size*par.pred_step)\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        # get batch of data\n",
    "#        if i == n_batches:\n",
    "#            # last batch\n",
    "#            x_batch, y_batch = create_batch(inputs,labels,remainder,-1/par.pred_step)\n",
    "#        else:\n",
    "        # batches before last\n",
    "        x_batch, y_batch = create_batch(inputs,labels,batch_size,i)\n",
    "\n",
    "        \n",
    "        # dropout at training\n",
    "        if par.dropout:\n",
    "            feed_dict_train = {x: x_batch, y: y_batch, input_kP: par.input_keepProb, \n",
    "                               output_kP: par.output_keepProb, recurrent_kP: par.recurrent_keepProb}\n",
    "        else:\n",
    "            feed_dict_train = {x: x_batch, y: y_batch, input_kP: 1.0, output_kP: 1.0, recurrent_kP: 1.0}\n",
    "            \n",
    "        # no dropout at testing and validation\n",
    "        feed_dict_test = {x: x_batch, y: y_batch, input_kP: 1.0, output_kP: 1.0, recurrent_kP: 1.0}\n",
    "            \n",
    "        # train the net on the data\n",
    "        if train:\n",
    "            prefix = \"Training_\" # for summary writer\n",
    "            session.run(optimizer,feed_dict=feed_dict_train) # run the optimization on the current batch\n",
    "#            loss_val, prediction_val, error_val = session.run(\n",
    "#                (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_train)\n",
    "        else:\n",
    "            prefix = \"Testing_\" # for summary writer\n",
    "#            loss_val, prediction_val, error_val = session.run(\n",
    "#                (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_test)\n",
    "            \n",
    "        loss_val, prediction_val, error_val = session.run(\n",
    "            (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_test)\n",
    "        \n",
    "        # prediction_val is a list of length \"par.pred_step\" with arrays of \"batch_size\" output values\n",
    "        # convert to numpy array of shape (batch_size, par.pred_step)\n",
    "        prediction_val = np.array(prediction_val)\n",
    "        # reshape the array to a vector of shape (1, par.pred_step*batch_size)\n",
    "        prediction_val = np.reshape(prediction_val, (1, par.pred_step*batch_size))\n",
    "        \n",
    "        loss_val_sum += loss_val # sum the losses across the batches\n",
    "        error_val_sum += error_val # sum the errors across the batches\n",
    "            \n",
    "        # save the results\n",
    "        if save:\n",
    "            # save the batch predictions to a list\n",
    "            prediction_list.extend(prediction_val[0,:])\n",
    "            \n",
    "    # the mean value of loss (throughout all the batches) in current epoch \n",
    "    loss_val_mean = loss_val_sum/n_batches        \n",
    "    # the mean of relative errors\n",
    "    error_val_mean = error_val_sum/n_batches\n",
    "        \n",
    "    # Create a new Summary object for sum of losses and mean of errors\n",
    "    loss_summary = tf.Summary()\n",
    "    error_summary = tf.Summary()\n",
    "    loss_summary.value.add(tag=\"{}Loss\".format(prefix), simple_value=loss_val_mean)\n",
    "    error_summary.value.add(tag=\"{}Error\".format(prefix), simple_value=error_val_mean)\n",
    "\n",
    "    # Add it to the Tensorboard summary writer\n",
    "    # Make sure to specify a step parameter to get nice graphs over time\n",
    "    summary_writer.add_summary(loss_summary, epoch)\n",
    "    summary_writer.add_summary(error_summary, epoch)\n",
    "\n",
    "    return loss_val_mean, error_val_mean, prediction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stopping(loss_val,epoch,stop_epochs,check_every):\n",
    "    \"\"\"\n",
    "    Save the model coefficients if the data loss function value is better than the last loss function value.\n",
    "\n",
    "    :param loss_val: current value of loss function\n",
    "    :param epoch: current epoch\n",
    "    :param stop_epochs: number of epochs after which the training is stopped if the loss is not improving\n",
    "    :param check_every: define the period in epochs at which to check the loss value (and at which to save the data)\n",
    "    :return: the epoch at which the best loss was and the value of the loss (ergo at which the last checkpoint was created)\n",
    "    \"\"\"\n",
    "    \n",
    "    stop_training = False\n",
    "    \n",
    "    # initialize function attributes\n",
    "    if not hasattr(early_stopping,\"best_loss\"):\n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.best_epoch = epoch\n",
    "    \n",
    "    # saving if the loss val is better than the last\n",
    "    if stop_epochs > 0 and epoch % check_every == 0:\n",
    "        # if loss val is better than best_loss save the model parameters\n",
    "        if loss_val < early_stopping.best_loss:\n",
    "            saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow') \n",
    "            early_stopping.best_loss = loss_val\n",
    "            early_stopping.best_epoch = epoch\n",
    "            print(\"Model saved at epoch {} \\nwith testing loss: {}.\".format(epoch,loss_val))\n",
    "        else:\n",
    "            print(\"Model NOT saved at epoch {} \\nwith testing loss: {}\".format(epoch,loss_val))\n",
    "            \n",
    "        print(\"____________________________\")\n",
    "    \n",
    "        # if the loss didn't improve for the last stop_epochs number of epochs then the training process will stop\n",
    "        if (epoch - early_stopping.best_epoch) >= stop_epochs:\n",
    "            stop_training = True \n",
    "    \n",
    "    # only saving\n",
    "    if stop_epochs <= 0 and epoch % check_every == 0:\n",
    "        saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "        early_stopping.best_loss = -1.0\n",
    "        early_stopping.best_epoch = -1\n",
    "        print(\"Model saved at epoch {}\".format(epoch))\n",
    "        print(\"____________________________\")\n",
    "        \n",
    "    \n",
    "    return early_stopping.best_loss, early_stopping.best_epoch, stop_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20180725-163854\n",
      "Epoch: 0\n",
      "TRAINING Loss: 0.3204911779583341\n",
      "TRAINING Error: 0.2831657771408422\n",
      "TESTING Loss: 0.20438391873324538\n",
      "TESTING Error: 0.23287321159521332\n",
      "Model saved at epoch 0 \n",
      "with testing loss: 0.20438391873324538.\n",
      "____________________________\n",
      "Model saved at epoch 10 \n",
      "with testing loss: 0.11635813762102094.\n",
      "____________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-61a0efc79dd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;31m# TESTING\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mloss_val_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror_val_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_batch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[1;31m# write the summaries of testing data at epoch in TensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-418ce796b768>\u001b[0m in \u001b[0;36mrun_model\u001b[1;34m(inputs, labels, batch_size, save, train)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         loss_val, prediction_val, error_val = session.run(\n\u001b[1;32m---> 55\u001b[1;33m             (loss, denormalized_prediction, SMAPE), feed_dict=feed_dict_test)\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;31m# prediction_val is a list of length \"par.pred_step\" with arrays of \"batch_size\" output values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAINING THE NETWORK\n",
    "\n",
    "# initializer of TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# define unique name for log directory (one-step and multi-step are in sepparate directories)\n",
    "now = datetime.now()\n",
    "if par.pred_step > 1:\n",
    "    logdir = \"./logs/multi-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "else:\n",
    "    logdir = \"./logs/one-step/\" + now.strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "\n",
    "print(now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "for repeat in range(par.n_repeats):\n",
    "#    with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        # take note of time at the start of training\n",
    "        time_train_start = time.time()\n",
    "        \n",
    "        # initialize helping variables\n",
    "        stop_training = False\n",
    "        best_epoch = par.n_epochs\n",
    "        \n",
    "        # reset instance variables\n",
    "        early_stopping.best_loss = 10000\n",
    "        early_stopping.best_epoch = 0\n",
    "\n",
    "        # Restore variables from disk if continueLearning is True.\n",
    "        if continueLearning:\n",
    "            # restoring variables will also initialize them\n",
    "            saver.restore(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "            print(\"Model restored.\")\n",
    "        else:\n",
    "            session.run(init) # initialize variables\n",
    "\n",
    "\n",
    "        # Create a SummaryWriter to output summaries and the Graph\n",
    "        # in console run 'tensorboard --logdir=./logs/'\n",
    "        summary_logdir = logdir + \"/\" + str(repeat)\n",
    "        summary_writer = tf.summary.FileWriter(logdir=summary_logdir, graph=session.graph)\n",
    "\n",
    "        for epoch in range(par.n_epochs):\n",
    "            # TRAINING\n",
    "            loss_val, error_val, _ = run_model(x_train,y_train,par.train_batch_size,save=False,train=True)\n",
    "\n",
    "            # TESTING\n",
    "            loss_val_test, error_val_test, _ = run_model(x_test,y_test,par.test_batch_size,save=False,train=False)\n",
    "\n",
    "            # write the summaries of testing data at epoch in TensorBoard\n",
    "    #        summary_writer.add_summary(summary_test, epoch)\n",
    "\n",
    "            # increment global step for decaying learning rate at each epoch\n",
    "            session.run(increment_global_step_op)\n",
    "\n",
    "            # Printing the results at every \"par.n_epochs//10\" epochs\n",
    "            if epoch % (par.n_epochs//10) == 0:\n",
    "                print(\"Epoch: {}\".format(epoch))\n",
    "                print(\"TRAINING Loss: {}\".format(loss_val))\n",
    "                print(\"TRAINING Error: {}\".format(error_val))\n",
    "                print(\"TESTING Loss: {}\".format(loss_val_test))\n",
    "                print(\"TESTING Error: {}\".format(error_val_test))\n",
    "                # flush the summary data into TensorBoard\n",
    "                # summary_writer.flush()\n",
    "\n",
    "            # Checking the model loss_value for early stopping each \"check_every\" epochs\n",
    "            # save the trained net and variables for later use if the test loss_val is better than the last saved one\n",
    "            best_loss, best_epoch, stop_training = early_stopping(loss_val_test,epoch,par.stop_epochs,par.check_every)\n",
    "\n",
    "            # Stop the training process\n",
    "            if stop_training:\n",
    "                print(\"The training process stopped prematurely at epoch {}.\".format(epoch))\n",
    "                break\n",
    "                \n",
    "        # take note of time at the end of training\n",
    "        time_train_end = time.time()\n",
    "        \n",
    "        # print training time per epoch\n",
    "        time_train = time_train_end - time_train_start\n",
    "        print(\"The training took {} seconds.\".format(time_train))\n",
    "        print(\"Average {} seconds/epoch\".format(time_train/epoch))\n",
    "\n",
    "\n",
    "    # Restoring the model coefficients with best results\n",
    "    with tf.Session() as session:\n",
    "\n",
    "        # restore the net coefficients with the lowest loss value\n",
    "        saver.restore(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow')\n",
    "        print('Restored model coefficients at epoch {} with TESTING loss val: {:.4f}'.format(best_epoch, best_loss))\n",
    "\n",
    "        # run the trained net with best coefficients on all time-series and save the results\n",
    "        loss_val, error_val, prediction_list = run_model(x_train,y_train,par.train_batch_size,save=True,train=False)\n",
    "        loss_val_test, error_val_test, prediction_list_test = run_model(x_test,y_test,par.test_batch_size,save=True,train=False)\n",
    "        loss_val_validation, error_val_validation, prediction_list_validation = run_model(x_validation,y_validation,par.val_batch_size,save=True,train=False)\n",
    "    \n",
    "\n",
    "    # printing parameters and results to console\n",
    "    results = '''Timestamp: {}\n",
    "_____________________________________________\n",
    "Net parameters:\n",
    "{}\n",
    "_____________________________________________\n",
    "Results: \\n\n",
    "Best epoch ...................... {:4d}\n",
    "TRAINING Loss ................... {:11.6f}\n",
    "TRAINING Error .................. {:11.6f}\n",
    "TESTING Loss .................... {:11.6f}\n",
    "TESTING Error ................... {:11.6f}\n",
    "VALIDATION Loss ................. {:11.6f}\n",
    "VALIDATION Error ................ {:11.6f}\n",
    "_____________________________________________'''.format(now.strftime(\"%Y%m%d-%H%M%S\")\n",
    "                                                       ,par\n",
    "                                                       ,best_epoch\n",
    "                                                       ,loss_val\n",
    "                                                       ,error_val\n",
    "                                                       ,loss_val_test\n",
    "                                                       ,error_val_test\n",
    "                                                       ,loss_val_validation\n",
    "                                                       ,error_val_validation)\n",
    "\n",
    "    print(results)\n",
    "\n",
    "\n",
    "    # saving results to log file in logdir\n",
    "    file = \"{}log{}.txt\".format(logdir,repeat)\n",
    "    with open(file, mode='w') as f:\n",
    "        f.write(results)\n",
    "\n",
    "    # Shift the predictions \"par.delay\" time-steps to the right\n",
    "    prediction_train = np.array(prediction_list)\n",
    "    prediction_test = np.array(prediction_list_test)\n",
    "    prediction_validation = np.array(prediction_list_validation)\n",
    "\n",
    "    print(np.shape(prediction_train))\n",
    "\n",
    "    prediction_train = np.pad(prediction_train,pad_width=((par.delay,0))\n",
    "                              ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "    prediction_test = np.pad(prediction_test,pad_width=((par.delay,0))\n",
    "                             ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "    prediction_validation = np.pad(prediction_validation,pad_width=((par.delay,0))\n",
    "                             ,mode='constant',constant_values=0) # pad with \"par.delay\" zeros at the start of first dimension\n",
    "\n",
    "    print(np.shape(prediction_train))\n",
    "\n",
    "\n",
    "    def denormalize(labels):\n",
    "        \"\"\"\n",
    "        Denormalize target values from interval (-1,1) to original values\n",
    "\n",
    "        :param labels: values of labels to be denormalized\n",
    "        :return: denormalized values of labels\n",
    "        \"\"\"\n",
    "\n",
    "        # denormalized_labels = min_data + labels*(max_data - min_data)\n",
    "        # denormalize the labels from (-1,1)\n",
    "        denormalized_labels = mean_data + labels*std_data\n",
    "\n",
    "        return denormalized_labels\n",
    "\n",
    "    y_train_denorm = denormalize(y_train)\n",
    "    y_test_denorm = denormalize(y_test)\n",
    "    y_validation_denorm = denormalize(y_validation)\n",
    "\n",
    "\n",
    "    # Plot the results\n",
    "    def plot_results(predictions, targets, selected_data):\n",
    "        \"\"\"\n",
    "        plot the predictions and target values to one figure\n",
    "\n",
    "        :param predictions: the predicted values from the net\n",
    "        :param targets: the actual values of the time series\n",
    "        :param selected_data: selector of input data (1 == Elbe flow, 2 == Saugeen flow) \n",
    "        :return: plot of predictions and target values\n",
    "        \"\"\"\n",
    "        plt.plot(predictions)\n",
    "        plt.plot(targets, alpha=0.6)\n",
    "        plt.legend(['predictions', 'targets'])\n",
    "        if selected_data == 1:\n",
    "            plt.xlabel('time (weeks)')\n",
    "            plt.ylabel('flow rate (normalized)')\n",
    "        elif selected_data == 2:\n",
    "            plt.xlabel('time (days)')\n",
    "            plt.ylabel('flow rate (unknown)')        \n",
    "        plt.draw()\n",
    "\n",
    "    f_training = plt.figure()\n",
    "    plot_results(prediction_train[plot_start:plot_end], y_train_denorm[plot_start:plot_end], selected_data)\n",
    "    plt.title('TRAINING')\n",
    "\n",
    "    f_testing = plt.figure()\n",
    "    plot_results(prediction_test[plot_start:plot_end], y_test_denorm[plot_start:plot_end], selected_data)\n",
    "    plt.title('TESTING')\n",
    "\n",
    "    f_validation = plt.figure()\n",
    "    plot_results(prediction_validation[plot_start:plot_end], y_validation_denorm[plot_start:plot_end], selected_data)\n",
    "    plt.title('VALIDATION')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the figures:\n",
    "    img_save_dir = \"{}IMG\".format(logdir)\n",
    "    save_dir_path = os.path.join(os.curdir, img_save_dir)\n",
    "    os.makedirs(save_dir_path, exist_ok=True)\n",
    "\n",
    "    f_training.savefig(save_dir_path + \"/training{}.pdf\".format(repeat), bbox_inches='tight')\n",
    "    f_testing.savefig(save_dir_path + \"/testing{}.pdf\".format(repeat), bbox_inches='tight')\n",
    "    f_validation.savefig(save_dir_path + \"/validation{}.pdf\".format(repeat), bbox_inches='tight')\n",
    "\n",
    "    print(\"Figures saved to: {}\".format(save_dir_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following cells are for debugging purposes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "par.pred_step = 4\n",
    "par.delay = 4\n",
    "\n",
    "x_batch, y_batch = create_batch(x_train,y_train,4,1)\n",
    "\n",
    "print(np.shape(x_batch),np.shape(y_batch))\n",
    "print(x_batch,'\\n----\\n', y_batch)\n",
    "\n",
    "x_batch, y_batch = create_batch(x_train,y_train,4,2)\n",
    "\n",
    "print(np.shape(x_batch),np.shape(y_batch))\n",
    "print(x_batch,'\\n-----\\n', y_batch)\n",
    "\n",
    "\n",
    "# x_train[7*(par.pred_step+par.delay-1):8*(par.pred_step+par.delay-1)]\n",
    "\n",
    "print(x_train[100:200:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    " - Variable length last batch to make use of all the available data!!!\n",
    " - plot every par.n_epochs//10 into tensorboard (different charts?)\n",
    " - try chaning par.hidden_size --- DONE\n",
    " - try chaning par.delay\n",
    " - try chaning par.n_lstm_layers\n",
    " - regularization (not necessarry)\n",
    "\n",
    "\n",
    " - test and validation --- DONE\n",
    " - drift??? --- DONE\n",
    " - Show the graph in TensorBoard -- DONE\n",
    " - add Dropout layer -- DONE\n",
    " - divide loss sum by n_iter to make it a loss val mean !!! --- DONE\n",
    " - add relative error between the predictions and targets --- DONE\n",
    "\n",
    "#### Hyperparameter tuning\n",
    " - smaller batch size is better because you have to predict less things before updating weights\n",
    "   but makes the training values more unstable\n",
    " - more prediction steps are worse\n",
    " - bigger delay?\n",
    " - number of stacked layers? - two is enough\n",
    " - smaller hidden size => faster, easier learning, less overfitting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "decayed_learning_rate_test = 0.01 * 0.90 ** (2000 / 1000\n",
    "print(decayed_learning_rate_test)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "par.pred_step = 1\n",
    "# print(create_batch(x_train,y_train, 21,-1//par.pred_step))\n",
    "\n",
    "print(x_train[-3:-1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# list of np arrays\n",
    "vectors = [np.array([1, 2, 3])]*5\n",
    "\n",
    "print(vectors)\n",
    "\n",
    "np_vectors = np.array(vectors)\n",
    "\n",
    "print(np.shape(np_vectors))\n",
    "\n",
    "np_vectors = np.reshape(np_vectors,(1,3*5))\n",
    "print(np_vectors)\n",
    "print(np.shape(np_vectors))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print(x_train)\n",
    "#print(n_iter)\n",
    "print(14)\n",
    "print((len(x_train)-par.delay)//(par.batch_size*par.pred_step))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.mean(x_train)\n",
    "print(np.shape(x_test), np.shape(x_test[:4000]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# print only tensor v1 in checkpoint file\n",
    "chkp.print_tensors_in_checkpoint_file('./checkpoints/Multi-Step_LSTMforPredictingLabeFlow',\n",
    "                                      tensor_name='output_layer/weights_out', \n",
    "                                      all_tensors=False, \n",
    "                                      all_tensor_names=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list_val = [1,2,3,4]\n",
    "\n",
    "pred_list = list(list_val[-i-1] for i in range(par.pred_step))\n",
    "pred_list = pred_list[::-1]\n",
    "\n",
    "print(pred_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Early stopping\n",
    "def early_stopping(loss_val,epoch):\n",
    "    \"\"\"\n",
    "    Save the model coefficients if the data loss function value is better than the last\n",
    "    loss function value.\n",
    "    Return the epoch at which the best loss was and the value of the loss (ergo at which the last checkpoint was created)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize function attributes\n",
    "    if not hasattr(early_stopping,\"best_loss\"):\n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.epoch = 0\n",
    "    \n",
    "    # compare the loss_val\n",
    "    if loss_val < early_stopping.best_loss:\n",
    " #       saver.save(session, './checkpoints/Multi-Step_LSTMforPredictingLabeFlow') \n",
    "        early_stopping.best_loss = loss_val\n",
    "        early_stopping.epoch = epoch\n",
    "        \n",
    "    return early_stopping.best_loss, early_stopping.epoch"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(early_stopping(0.5,0))\n",
    "print(early_stopping(0.6,10))\n",
    "print(early_stopping(0.4,20))\n",
    "print(early_stopping(0.8,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prediction_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_validation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# define net parameters\n",
    "class params:\n",
    "    # initialization of instance variables\n",
    "    def __init__(self,n_lstm_layers,hidden_size,delay,pred_step,batch_size,n_epochs,stop_epochs,init_lr):\n",
    "        self.input_size = 1 # number of input features (we only follow one variable == flow)\n",
    "        self.num_classes = 1 # number of output classes (we wan't specific value, not classes, so this is 1)\n",
    "        self.target_shift = 1 # the target is the same time-series shifted by 1 time-step forward\n",
    "        self.n_lstm_layers = n_lstm_layers # number of vertically stacked LSTM layers\n",
    "        self.hidden_size = hidden_size # hidden state vector size in LSTM cell\n",
    "        self.delay = delay # the number of time-steps from which we are going to predict the next step\n",
    "        self.pred_step = pred_step # the number of time-steps we predict into the future (1 == One-step prediction ; >1 == Multi-step prediction)\n",
    "        self.batch_size = batch_size # number of inputs in one batch\n",
    "        self.n_epochs = n_epochs # number of epochs\n",
    "        self.stop_epochs = stop_epochs # if the loss value doesn't improve over the last stop_epochs, the training process will stop\n",
    "        self.init_lr = init_lr # initial learning rate for Adam optimizer (training algorithm)\n",
    "        self.net_unroll_size = delay + pred_step - 1 # number of unrolled LSTM time-step cells\n",
    "        \n",
    "    # how will the class object be represented in string form (eg. when called with print())\n",
    "    def __str__(self):\n",
    "        answer = '''\n",
    "Input size ...................... {:4d}\n",
    "Number of classes ............... {:4d}\n",
    "Target shift .................... {:4d}\n",
    "Number of stacked LSTM layers ... {:4d}\n",
    "Hidden state size ............... {:4d}\n",
    "Delay ........................... {:4d}\n",
    "Number of prediciton steps....... {:4d}\n",
    "Batch size ...................... {:4d}\n",
    "Maximum number of epochs ........ {:4d}\n",
    "Early stopping epochs ........... {:4d}\n",
    "Initial learning rate ........... {:9.4f}'''.format(self.input_size\n",
    "                                              ,self.num_classes\n",
    "                                              ,self.target_shift\n",
    "                                              ,self.n_lstm_layers\n",
    "                                              ,self.hidden_size\n",
    "                                              ,self.delay\n",
    "                                              ,self.pred_step\n",
    "                                              ,self.batch_size\n",
    "                                              ,self.n_epochs\n",
    "                                              ,self.stop_epochs\n",
    "                                              ,self.init_lr)\n",
    "        return answer\n",
    "        # return str(vars(self))\n",
    "\n",
    "# net and training parameter specification\n",
    "par = params(2,20,10,1,5,100,20,0.001)\n",
    "\n",
    "print(par)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p1 = params()\n",
    "p2 = params()\n",
    "p1.par.batch_size = 10\n",
    "p2.par.batch_size = 20\n",
    "\n",
    "print(p1.par.batch_size, p2.par.batch_size)\n",
    "\n",
    "p1.par.delay = 5\n",
    "\n",
    "print(p1.par.delay)\n",
    "print(p1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "variables = vars(p1)\n",
    "#print(variables)\n",
    "print(\"Params: {}\".format(p1))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "p1.par.input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(10)\n",
    "\n",
    "print(x)\n",
    "\n",
    "mov_avg = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "    mov_avg = (mov_avg + x[i])/(i+1)\n",
    "    \n",
    "error = np.sum(x)/len(x)\n",
    "\n",
    "print(mov_avg, error)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data_size = 208\n",
    "par.batch_size = 4\n",
    "par.delay = 51\n",
    "par.pred_step = 3\n",
    "\n",
    "n_batches = (len(x_train)-par.delay-2)//(par.batch_size*par.pred_step)\n",
    "\n",
    "print(create_batch(x_train, y_train, par.batch_size,n_batches))\n",
    "\n",
    "print(y_train[-2])\n",
    "\n",
    "index = 0\n",
    "i = 0\n",
    "step = index*(batch_size*par.pred_step)\n",
    "\n",
    "y_batch = np.reshape(y_train[step+par.delay+i*par.pred_step+1:step+par.delay+i*par.pred_step+1+par.pred_step],(1,par.num_classes,par.pred_step))\n",
    "\n",
    "print(y_batch, y_train[40:51])\n",
    "\n",
    "\n",
    "print(n_batches)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
